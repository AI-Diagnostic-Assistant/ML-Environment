{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "LxEzv2W5oQCh",
        "mu_y9CbKW3e8",
        "ZaofyY3kXQfp",
        "0pqe7vjlXaBW",
        "lUf3B6xMkfGG",
        "bgLXZUzorn5W",
        "1ggqmRWRXkM-",
        "UWqIj4GUSCts",
        "KhHVZ93uBjh4",
        "HwOeg7O7lf5h",
        "qMqVXd6JkeVX",
        "fI7zS684mZKo",
        "l_bIqzXFAEw4",
        "rayKlaxUJYNq",
        "qvthBNKOIe7y",
        "RHu-jo5bZF0R",
        "15kINhY_RySh",
        "Vl7ygJj9y8xA",
        "TbYg7vF9C1UK",
        "NuRbAcYy1uLY"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI-Diagnostic-Assistant/ML-Environment/blob/main/UTO_XAI_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installations**"
      ],
      "metadata": {
        "id": "LxEzv2W5oQCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom\n",
        "!pip install segmentation-models-pytorch\n",
        "!pip install --upgrade --force-reinstall --no-deps segmentation-models-pytorch\n",
        "!pip install catalyst\n",
        "!pip install lightgbm\n"
      ],
      "metadata": {
        "id": "-AuWK0fl_YG5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports and config**"
      ],
      "metadata": {
        "id": "mu_y9CbKW3e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import pydicom\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from matplotlib.patches import Rectangle\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import segmentation_models_pytorch as smp\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import cv2\n",
        "import json\n",
        "from skimage.transform import resize\n",
        "from pydicom.uid import generate_uid\n",
        "from pydicom.dataset import FileDataset\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shap\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, make_scorer, recall_score, precision_score, f1_score, roc_curve, auc\n",
        "from scipy.stats import kurtosis, skew\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, cross_validate\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import random\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from scipy.interpolate import CubicSpline\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import functools\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from collections import defaultdict\n",
        "from sklearn.pipeline import Pipeline\n",
        "from collections import Counter\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.interpolate import UnivariateSpline\n",
        "from sklearn.base import clone\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from joblib import dump, load\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "parent_dir_ckd = '/content/drive/My Drive/drs_dataset/DATA_DICOM'\n",
        "parent_dir_uto = '/content/drive/My Drive/uto/uto_dataset/DATA_DICOM'\n",
        "\n",
        "clinical_data_dir = '/content/drive/My Drive/drs_dataset/TABLES_b'\n",
        "masked_composite_images = '/content/drive/My Drive/masked_composite_images'\n",
        "composite_images_dir = '/content/drive/My Drive/composite_images'\n",
        "composite_images_dir_uto = '/content/drive/My Drive/uto/composite_images'\n",
        "\n",
        "kidney_rois_dir= '/content/drive/My Drive/masked_kidneys'\n",
        "kidney_rois_uto_dir= '/content/drive/My Drive/uto/masked_kidneys_uto'\n",
        "\n",
        "masked_frames_dir= '/content/drive/My Drive/masked_frames'\n",
        "masked_frames_summed_dir= '/content/drive/My Drive/masked_frames_summed'\n",
        "renograms_summed_raw_dir= '/content/drive/My Drive/uto/renograms_summed'\n",
        "renograms_raw_dir= '/content/drive/My Drive/uto/renograms_raw'\n",
        "renograms_raw_test_dir= '/content/drive/My Drive/renograms_raw_test'\n",
        "renograms_raw_test_inter_nearest_dir= '/content/drive/My Drive/renograms_raw_test_inter_nearest'\n",
        "\n",
        "renograms_summed_raw_sum_dir= '/content/drive/My Drive/uto/renograms_summed_gug'\n",
        "renograms_raw_sum_dir= '/content/drive/My Drive/renograms_raw_sum'\n",
        "masked_frames_test_dir= '/content/drive/My Drive/masked_frames_test'\n",
        "\n",
        "renograms_dir= '/content/drive/My Drive/renograms'\n",
        "renograms_summed_dir= '/content/drive/My Drive/renograms_summed'\n",
        "renograms_unormalized_dir= '/content/drive/My Drive/renograms_unormalized'\n",
        "patient_labels_dir = '/content/drive/My Drive/patient_labels'\n",
        "patient_labels_uto_dir = '/content/drive/My Drive/uto/uto_dataset/patient_labels'\n",
        "masked_frames_dicom_dir = '/content/drive/My Drive/masked_frames_dicom'\n",
        "segmentation_models= '/content/drive/My Drive/segmentation_models'\n",
        "segmentation_models_test= '/content/drive/My Drive/segmentation_models_test'\n",
        "\n",
        "evaluation_metrics_results_unet = \"/content/drive/My Drive/evaluation_metrics/unet/evaluation_metrics.json\"\n",
        "evaluation_metrics_results_svm = \"/content/drive/My Drive/uto/models/svm_models/evaluation_metrics.json\"\n",
        "evaluation_metrics_results_dt = \"/content/drive/My Drive/uto/models/dt_models/evaluation_metrics.json\"\n",
        "evaluation_metrics_results_xgboost = \"/content/drive/My Drive/uto/models/xgboost_models/evaluation_metrics.json\"\n",
        "evaluation_metrics_results_rf = \"/content/drive/My Drive/uto/models/rf_models/evaluation_metrics.json\"\n",
        "evaluation_metrics_results_knn = \"/content/drive/My Drive/uto/models/knn_models/evaluation_metrics.json\"\n",
        "evaluation_metrics_results_ens = \"/content/drive/My Drive/uto/models/ensamble_models/evaluation_metrics.json\"\n",
        "\n",
        "\n",
        "svm_model_dir = '/content/drive/MyDrive/uto/models/svm_models'\n",
        "dataset_analysis_plots_dir = '/content/drive/MyDrive/uto/uto_dataset/analysis'\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qaBiM33xW8Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Dataset classes**"
      ],
      "metadata": {
        "id": "ZaofyY3kXQfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, images, left_masks, right_masks):\n",
        "        self.images = images\n",
        "        self.left_masks = left_masks\n",
        "        self.right_masks = right_masks\n",
        "        #self.masks = masks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        left_mask = self.left_masks[idx]\n",
        "        right_mask = self.right_masks[idx]\n",
        "        #mask = self.masks[idx]\n",
        "\n",
        "        # Convert to tensors\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        left_mask = torch.tensor(left_mask, dtype=torch.long)\n",
        "        right_mask = torch.tensor(right_mask, dtype=torch.long)\n",
        "        #mask = torch.tensor(mask, dtype=torch.long)\n",
        "\n",
        "        # Transpose image to (C, H, W)\n",
        "        if image.ndim == 3:\n",
        "            image = image.permute(2, 0, 1)  # PyTorch tensor permute\n",
        "\n",
        "        # Ensure masks have a channel dimension\n",
        "        if left_mask.ndim == 2:\n",
        "            left_mask = left_mask.unsqueeze(0)\n",
        "        elif left_mask.ndim == 3 and left_mask.shape[2] == 1:\n",
        "            left_mask = left_mask.permute(2, 0, 1)\n",
        "\n",
        "        if right_mask.ndim == 2:\n",
        "            right_mask = right_mask.unsqueeze(0)\n",
        "        elif right_mask.ndim == 3 and right_mask.shape[2] == 1:\n",
        "            right_mask = right_mask.permute(2, 0, 1)\n",
        "\n",
        "        return image, left_mask, right_mask\n",
        "\n",
        "\n",
        "class RenogramDatasetUTO(Dataset):\n",
        "  def __init__(self, activities, labels, dicom_dir):\n",
        "        self.activities = []\n",
        "        self.time_vectors  = []\n",
        "        self.labels = []\n",
        "        self.diuretic_times = []\n",
        "        self.patient_ids = []\n",
        "\n",
        "        # Loop over each patient\n",
        "        for pid, activity in activities.items():\n",
        "            if pid in labels:\n",
        "                # Assuming labels[patient_folder] is a tuple (left_label, right_label)\n",
        "                left_label, right_label, diuretic_time = labels[pid]\n",
        "\n",
        "                t_min = np.array(activity[\"times_min\"], dtype=float)\n",
        "\n",
        "                # Append left kidney sample and label\n",
        "                self.activities.append(np.array(activity[\"left\"]))\n",
        "                self.time_vectors.append(t_min)\n",
        "                self.labels.append(left_label)\n",
        "                self.diuretic_times.append(diuretic_time)\n",
        "                self.patient_ids.append(pid)\n",
        "\n",
        "                # Append right kidney sample and label\n",
        "                self.activities.append(np.array(activity[\"right\"]))\n",
        "                self.time_vectors.append(t_min)\n",
        "                self.labels.append(right_label)\n",
        "                self.diuretic_times.append(diuretic_time)\n",
        "                self.patient_ids.append(pid)\n",
        "\n",
        "\n",
        "        # Convert labels to a tensor\n",
        "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.activities)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        # Return a single kidney curve and its corresponding label\n",
        "        return self.activities[idx], self.labels[idx], self.diuretic_times[idx], self.patient_ids[idx], self.time_vectors[idx]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rIEJEo6xXTVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "0pqe7vjlXaBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_uptake_composite(dcm_folder_path,\n",
        "                            file_pattern,\n",
        "                            patient_folder,\n",
        "                            target_size=(128,128),\n",
        "                            uptake_window=(120,180)):\n",
        "\n",
        "    patt = re.compile(file_pattern)\n",
        "    files = sorted(f for f in os.listdir(dcm_folder_path) if patt.match(f))\n",
        "    if not files:\n",
        "        print(f\"⚠️ No files matched {file_pattern} in {dcm_folder_path}\")\n",
        "        return None\n",
        "\n",
        "    ds = pydicom.dcmread(os.path.join(dcm_folder_path, files[0]))\n",
        "    arr = ds.pixel_array\n",
        "    n_frames = arr.shape[0]\n",
        "\n",
        "    try:\n",
        "        phases = ds.PhaseInformationSequence\n",
        "    except AttributeError:\n",
        "        print(\"⚠️ DICOM has no PhaseInformationSequence; can't build time axis\")\n",
        "        return None\n",
        "\n",
        "    durations_ms = []\n",
        "    for item in phases:\n",
        "        dur = float(item.ActualFrameDuration)\n",
        "        count = int(item.NumberOfFramesInPhase)\n",
        "        durations_ms.extend([dur] * count)\n",
        "\n",
        "    durations_ms = (durations_ms + durations_ms)[ : n_frames ]\n",
        "\n",
        "    cum_ms = np.cumsum(durations_ms)\n",
        "    rel_s = (cum_ms - durations_ms[0]) / 1000.0\n",
        "\n",
        "    idxs = np.where((rel_s >= uptake_window[0]) &\n",
        "                    (rel_s <  uptake_window[1]))[0]\n",
        "    if idxs.size == 0:\n",
        "        print(f\"No frames in {uptake_window[0]}–{uptake_window[1]} s for {patient_folder!r}\")\n",
        "        return None\n",
        "\n",
        "    comp = np.sum(arr[idxs, ...], axis=0)\n",
        "\n",
        "    mx = comp.max()\n",
        "    if mx > 0:\n",
        "        comp = comp / mx\n",
        "    else:\n",
        "        print(f\"Uptake composite all zeros for {patient_folder!r}\")\n",
        "        return None\n",
        "    comp = np.nan_to_num(comp, nan=0.0)\n",
        "\n",
        "    if comp.shape != target_size:\n",
        "        comp = cv2.resize(\n",
        "            comp,\n",
        "            dsize=target_size[::-1],\n",
        "            interpolation=cv2.INTER_LINEAR\n",
        "        )\n",
        "        comp -= comp.min()\n",
        "        m2 = comp.max()\n",
        "        if m2 > 0:\n",
        "            comp = comp / m2\n",
        "\n",
        "    out_fn   = f'composite_image_{patient_folder}.png'\n",
        "    out_path = os.path.join(composite_images_dir_uto, out_fn)\n",
        "    plt.imsave(\n",
        "        out_path,\n",
        "        comp,\n",
        "        cmap='gray',\n",
        "        vmin=0.0,\n",
        "        vmax=1.0,\n",
        "        format='png'\n",
        "    )\n",
        "    print(f\"Saved uptake-phase composite to {out_path}\")\n",
        "    return comp\n",
        "\n",
        "\n",
        "\n",
        "def load_composite_images_with_roi_labels(image_dir, masked_kidneys_dir, img_height=128, img_width=128, preprocessing_fn=None):\n",
        "    image_filenames = os.listdir(image_dir)\n",
        "    images = []\n",
        "    left_masks = []\n",
        "    right_masks = []\n",
        "\n",
        "    for filename in image_filenames:\n",
        "        image_path = os.path.join(image_dir, filename)\n",
        "        patient_id = filename.split('_')[-1].split('.')[0]\n",
        "\n",
        "        if patient_id.isdigit():\n",
        "            left_mask_filename = f\"left_kidney_mask_drsprg_{patient_id}.png\"\n",
        "            right_mask_filename = f\"right_kidney_mask_drsprg_{patient_id}.png\"\n",
        "        else:\n",
        "            left_mask_filename = f\"left_kidney_mask_{patient_id}.png\"\n",
        "            right_mask_filename = f\"right_kidney_mask_{patient_id}.png\"\n",
        "\n",
        "        left_mask_path = os.path.join(masked_kidneys_dir, left_mask_filename)\n",
        "        right_mask_path = os.path.join(masked_kidneys_dir, right_mask_filename)\n",
        "\n",
        "        if not os.path.exists(left_mask_path) or not os.path.exists(right_mask_path):\n",
        "          print(f\"Did not find match for {filename}\")\n",
        "          continue\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.resize(image, (img_width, img_height))\n",
        "\n",
        "        if preprocessing_fn is not None:\n",
        "            image = preprocessing_fn(image)\n",
        "        else:\n",
        "            image = image.astype(np.float32) / 255.0\n",
        "\n",
        "        left_mask = cv2.imread(left_mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        left_mask = cv2.resize(left_mask, (img_width, img_height))\n",
        "        left_mask = (left_mask > 127).astype(np.float32)\n",
        "        left_mask = np.expand_dims(left_mask, axis=-1)\n",
        "\n",
        "        right_mask = cv2.imread(right_mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        right_mask = cv2.resize(right_mask, (img_width, img_height))\n",
        "        right_mask = (right_mask > 127).astype(np.float32)\n",
        "        right_mask = np.expand_dims(right_mask, axis=-1)\n",
        "\n",
        "        images.append(image)\n",
        "        left_masks.append(left_mask)\n",
        "        right_masks.append(right_mask)\n",
        "\n",
        "    images = np.array(images)\n",
        "    left_masks = np.array(left_masks)\n",
        "    right_masks = np.array(right_masks)\n",
        "\n",
        "    return images, left_masks, right_masks\n",
        "\n",
        "def total_loss(outputs, targets):\n",
        "    dice_loss = smp.losses.DiceLoss(mode='binary')\n",
        "    focal_loss = smp.losses.FocalLoss(mode='binary')\n",
        "\n",
        "    loss1 = dice_loss(outputs, targets)\n",
        "    loss2 = focal_loss(outputs, targets)\n",
        "    return loss1 + loss2\n",
        "\n",
        "\n",
        "def pretrained_unet_model(backbone='resnet34', lr=1e-4):\n",
        "    n_classes = 2\n",
        "    activation = None\n",
        "\n",
        "    # Create the model\n",
        "    model = smp.Unet(\n",
        "        encoder_name=backbone,\n",
        "        encoder_weights='imagenet',\n",
        "        in_channels=3,\n",
        "        classes=n_classes,\n",
        "        activation=activation\n",
        "    )\n",
        "\n",
        "    # Define optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Define loss function\n",
        "    criterion = total_loss\n",
        "\n",
        "    # Metrics\n",
        "    metric_funcs = {\n",
        "        'iou_score': smp.metrics.functional.iou_score,\n",
        "        'fscore': smp.metrics.f1_score\n",
        "    }\n",
        "\n",
        "    return model, optimizer, criterion, metric_funcs\n",
        "\n",
        "\n",
        "\n",
        "def save_evaluation_metrics_unet(all_folds_results):\n",
        "    results_to_save = {\n",
        "        \"fold_results\": {\n",
        "            fold: {\n",
        "                \"iou_score\": all_folds_results[fold]['iou_score'],\n",
        "                \"dice_score\": all_folds_results[fold]['dice_score']\n",
        "            } for fold in range(1, n_splits + 1)\n",
        "        },\n",
        "        \"mean_scores\": {\n",
        "            \"mean_iou_score\": mean_iou_score_over_folds,\n",
        "            \"mean_dice_score\": mean_dice_score_over_folds\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(evaluation_metrics_results_unet, 'w') as json_file:\n",
        "        json.dump(results_to_save, json_file, indent=4)\n",
        "\n",
        "\n",
        "def trim_bad_by_fraction(left, right, times, frac_thresh=0.8, span=5):\n",
        "    df = pd.DataFrame({\"L\": left, \"R\": right})\n",
        "\n",
        "    roll_L = df[\"L\"].rolling(span, min_periods=1).mean()\n",
        "    roll_R = df[\"R\"].rolling(span, min_periods=1).mean()\n",
        "\n",
        "    # drop until last frame is ≥ threshold × rolling mean\n",
        "    while len(df) > 1:\n",
        "        if (df[\"L\"].iloc[-1] < frac_thresh * roll_L.iloc[-1]\n",
        "            or df[\"R\"].iloc[-1] < frac_thresh * roll_R.iloc[-1]):\n",
        "            df = df.iloc[:-1]\n",
        "            roll_L = roll_L.iloc[:-1]; roll_R = roll_R.iloc[:-1]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return df[\"L\"].to_numpy(), df[\"R\"].to_numpy(), times[: len(df)]\n",
        "\n",
        "\n",
        "def create_renograms_raw(dcm_folder_path, file_pattern, patient_folder, left_kidney_mask, right_kidney_mask, all_activites):\n",
        "\n",
        "    output_dir = os.path.join(renograms_raw_dir, patient_folder)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    dicom_files = [file for file in os.listdir(dcm_folder_path) if re.match(file_pattern, file)]\n",
        "\n",
        "    if not dicom_files:\n",
        "        print(f\"No DICOM files found in {dcm_folder_path} matching {file_pattern}.\")\n",
        "        return\n",
        "\n",
        "    left_activities, right_activities = [], []\n",
        "    time_s_list = []\n",
        "\n",
        "\n",
        "    for dcm_file in dicom_files:\n",
        "        dcm_file_path = os.path.join(dcm_folder_path, dcm_file)\n",
        "        print(f\"Processing DICOM file: {dcm_file}\")\n",
        "\n",
        "        ds = pydicom.dcmread(dcm_file_path)\n",
        "\n",
        "        if hasattr(ds, \"PhaseInformationSequence\"):\n",
        "            frame_times_ms = []\n",
        "            for phase in ds.PhaseInformationSequence:\n",
        "                dur_ms = float(phase.ActualFrameDuration)\n",
        "                nfr   = int(phase.NumberOfFramesInPhase)\n",
        "                frame_times_ms += [dur_ms] * nfr\n",
        "            frame_times_s = np.array(frame_times_ms, dtype=float) / 1000.0\n",
        "        else:\n",
        "            frame_times_s = np.full(int(ds.NumberOfFrames), 10.0)\n",
        "\n",
        "        time_s_list.extend(frame_times_s)\n",
        "\n",
        "        # Ensure masks are in proper format\n",
        "        if left_kidney_mask.max() <= 1.0:\n",
        "            left_kidney_mask = (left_kidney_mask * 255).astype(np.uint8)\n",
        "        if right_kidney_mask.max() <= 1.0:\n",
        "            right_kidney_mask = (right_kidney_mask * 255).astype(np.uint8)\n",
        "\n",
        "        # Ensure masks match image size\n",
        "        resized_left_kidney_roi = cv2.resize(left_kidney_mask, (ds.pixel_array.shape[2], ds.pixel_array.shape[1]), interpolation=cv2.INTER_LINEAR)\n",
        "        resized_right_kidney_roi = cv2.resize(right_kidney_mask, (ds.pixel_array.shape[2], ds.pixel_array.shape[1]), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        for frame_idx in range(ds.NumberOfFrames):\n",
        "            pixel_array = ds.pixel_array[frame_idx].astype(np.float32)\n",
        "            dur   = frame_times_s[frame_idx]\n",
        "\n",
        "            # Apply masks to raw pixel values\n",
        "            left_masked_frame = cv2.bitwise_and(pixel_array, pixel_array, mask=resized_left_kidney_roi)\n",
        "            right_masked_frame = cv2.bitwise_and(pixel_array, pixel_array, mask=resized_right_kidney_roi)\n",
        "\n",
        "            # Compute activity\n",
        "            left_activity = compute_activity(left_masked_frame, dur)\n",
        "            right_activity = compute_activity(right_masked_frame, dur)\n",
        "\n",
        "            left_activities.append(left_activity)\n",
        "            right_activities.append(right_activity)\n",
        "\n",
        "\n",
        "    #Cut of last frame if below treshold\n",
        "    times_arr  = np.cumsum(np.array(time_s_list))\n",
        "    left_arr   = np.array(left_activities, dtype=float)\n",
        "    right_arr  = np.array(right_activities, dtype=float)\n",
        "\n",
        "    # apply the drop‐last‐if‐bad to each kidney\n",
        "    left_arr, right_arr, times_arr = trim_bad_by_fraction(left_arr, right_arr, times_arr, 0.8, 6)\n",
        "\n",
        "    time_min = times_arr / 60.0\n",
        "\n",
        "    left_activities  = left_arr.tolist()\n",
        "    right_activities = right_arr.tolist()\n",
        "\n",
        "    # Store activities in dictionary\n",
        "    all_activities[patient_folder] = {\n",
        "        \"times_s\": times_arr.tolist(),\n",
        "        \"times_min\": (times_arr/60.0).tolist(),\n",
        "        \"left\":    left_activities,\n",
        "        \"right\":   right_activities,\n",
        "    }\n",
        "\n",
        "    save_renogram(left_activities, right_activities, time_min, output_dir)\n",
        "\n",
        "\n",
        "def save_renogram(left_acts, right_acts, time_min, output_dir):\n",
        "  fig, ax = plt.subplots(figsize=(10,4))\n",
        "  ax.plot(time_min, left_acts,  label=\"Left kidney\")\n",
        "  ax.plot(time_min, right_acts, label=\"Right kidney\")\n",
        "  ax.xaxis.set_major_locator(MultipleLocator(1))\n",
        "  plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "  ax.set_xlabel(\"Time (minutes)\")\n",
        "  ax.set_ylabel(\"counts/sec\")\n",
        "  ax.legend(loc=\"best\")\n",
        "  ax.grid(True)\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(os.path.join(output_dir, \"renogram.png\"))\n",
        "  plt.close()\n",
        "\n",
        "def get_one_predicted_mask(image_tensor):\n",
        "    with torch.no_grad():\n",
        "        pred_mask = model(image_tensor)\n",
        "        pred_mask = (pred_mask > 0.5).cpu().numpy().squeeze()\n",
        "    return pred_mask\n",
        "\n",
        "\n",
        "def load_image(image_path, img_height=256, img_width=256, preprocessing_fn=None):\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
        "\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    image = cv2.resize(image, (img_width, img_height))\n",
        "\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Failed to load the image at {image_path}\")\n",
        "\n",
        "    if preprocessing_fn is not None:\n",
        "        image = preprocessing_fn(image)\n",
        "    else:\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def compute_activity(masked_array, frame_duration):\n",
        "    total_counts = np.sum(masked_array)\n",
        "    return total_counts / frame_duration\n",
        "\n",
        "def compute_t_half(curve: np.ndarray, time_vector: np.ndarray, diuretic_time: float):\n",
        "    t = time_vector\n",
        "    n = len(curve)\n",
        "\n",
        "    inj_idx = np.searchsorted(t, diuretic_time, side='right') - 1\n",
        "    inj_ct = curve[inj_idx] if 0 <= inj_idx < n else np.nan\n",
        "\n",
        "    dh = np.nan\n",
        "    if 0 <= inj_idx < n and not np.isnan(inj_ct):\n",
        "        thr = inj_ct / 2.0\n",
        "        for i in range(inj_idx, n):\n",
        "            if curve[i] <= thr:\n",
        "                dh = t[i] - diuretic_time\n",
        "                break\n",
        "\n",
        "    return dh\n",
        "\n",
        "\n",
        "\n",
        "def prepare_training_data(data_loader, training_mode=\"datapoint\"):\n",
        "    X = []\n",
        "    y = []\n",
        "    injection_times = []\n",
        "    patients = []\n",
        "    time_vectors = []\n",
        "\n",
        "    for activities, labels, diuretic_times, patient_folders, batch_time_vectors in data_loader:\n",
        "        for i in range(len(labels)):\n",
        "            X.append(activities[i])\n",
        "            y.append(labels[i].item())\n",
        "            injection_times.append(diuretic_times[i])\n",
        "            patients.append(patient_folders[i])\n",
        "            time_vectors.append(batch_time_vectors[i])\n",
        "\n",
        "    if training_mode == \"feature\":\n",
        "      X = list(X)\n",
        "\n",
        "    y = np.array(y)\n",
        "    injection_times = np.array(injection_times)\n",
        "    patients = np.array(patients)\n",
        "    time_vectors = np.array(time_vectors, dtype=object)\n",
        "\n",
        "    return X, y, injection_times, patients, time_vectors\n",
        "\n",
        "\n",
        "def statistical_features(curve: np.ndarray):\n",
        "    return [\n",
        "        np.mean(curve),\n",
        "        np.var(curve),\n",
        "        skew(curve),\n",
        "        kurtosis(curve),\n",
        "    ]\n",
        "\n",
        "\n",
        "def related_work_features(curve: np.ndarray, time_vector: np.ndarray, diuretic_time: float):\n",
        "    if hasattr(curve, \"cpu\"):\n",
        "        curve = curve.cpu().detach().numpy()\n",
        "\n",
        "    t = time_vector\n",
        "    n = len(curve)\n",
        "\n",
        "    # mean/statistics\n",
        "    m = np.mean(curve)\n",
        "    v = np.var(curve)\n",
        "    sk = skew(curve)\n",
        "    kt = kurtosis(curve)\n",
        "\n",
        "    end_time = t[-1]\n",
        "    covers_5  = end_time >= diuretic_time + 5.0\n",
        "    covers_20 = end_time >= diuretic_time + 20.0\n",
        "\n",
        "    # find indices\n",
        "    inj_idx = np.searchsorted(t, diuretic_time, side='right') - 1\n",
        "    f30_idx = np.searchsorted(t, 30.0, side='right') - 1\n",
        "    post5_idx   = np.searchsorted(t, diuretic_time + 5.0, side='right') - 1\n",
        "    post15_idx  = np.searchsorted(t, diuretic_time + 15.0, side='right') - 1\n",
        "    post20_idx  = np.searchsorted(t, diuretic_time + 20.0, side='right') - 1\n",
        "\n",
        "    #C_last as a replacement for C30 due to variable length of sequences.\n",
        "    last_idx = len(curve) - 1\n",
        "    C_last = 100.0 * (curve[inj_idx] - curve[last_idx]) / curve[inj_idx]\n",
        "\n",
        "     # mean slope during first 5 min post-furo\n",
        "    if covers_5 and post5_idx > inj_idx:\n",
        "        ds = np.diff(curve[inj_idx:post5_idx+1])\n",
        "        dt = np.diff(t[inj_idx:post5_idx+1])\n",
        "        slope_0_5 = np.mean(ds / dt)\n",
        "    else:\n",
        "        slope_0_5 = np.nan\n",
        "\n",
        "    # mean slope between 15–20 min post-furo\n",
        "    if covers_20 and post20_idx > post15_idx:\n",
        "        ds = np.diff(curve[post15_idx:post20_idx+1])\n",
        "        dt = np.diff(t[post15_idx:post20_idx+1])\n",
        "        slope_15_20 = np.mean(ds / dt)\n",
        "    else:\n",
        "        slope_15_20 = np.nan\n",
        "\n",
        "    # Curve length: arc-length from injection to end on smoothed curve\n",
        "    ds_len = np.diff(curve[inj_idx:])\n",
        "    dt_len = np.diff(t[inj_idx:])\n",
        "    if len(ds_len) > 0:\n",
        "        length = np.sum(np.sqrt(ds_len**2 + dt_len**2))\n",
        "    else:\n",
        "        length = np.nan\n",
        "\n",
        "    def _fix(x): return -1 if np.isnan(x) else x\n",
        "\n",
        "    return [\n",
        "        m, v, _fix(sk), _fix(kt), _fix(C_last), _fix(slope_0_5), _fix(slope_15_20), _fix(length)\n",
        "    ]\n",
        "\n",
        "def domain_expertise_features(curve: np.ndarray, time_vector: np.ndarray, diuretic_time: float):\n",
        "    if hasattr(curve, \"cpu\"):\n",
        "        curve = curve.cpu().detach().numpy()\n",
        "\n",
        "    t = time_vector\n",
        "    n = len(curve)\n",
        "\n",
        "    peak_idx = np.argmax(curve)\n",
        "    peak_ct = curve[peak_idx]\n",
        "    ttp = t[peak_idx]\n",
        "\n",
        "    inj_idx = np.searchsorted(t, diuretic_time, side='right') - 1\n",
        "    inj_ct = curve[inj_idx] if 0 <= inj_idx < n else np.nan\n",
        "\n",
        "    # baseline half-time\n",
        "    bh = np.nan\n",
        "    if peak_idx < inj_idx:\n",
        "        for i in range(peak_idx, inj_idx+1):\n",
        "            if curve[i] <= peak_ct/2.0:\n",
        "                bh = t[i] - ttp\n",
        "                break\n",
        "\n",
        "    # diuretic half-time\n",
        "    dh = np.nan\n",
        "    if 0 <= inj_idx < n and not np.isnan(inj_ct):\n",
        "        thr = inj_ct / 2.0\n",
        "        for i in range(inj_idx, n):\n",
        "            if curve[i] <= thr:\n",
        "                dh = t[i] - diuretic_time\n",
        "                break\n",
        "\n",
        "\n",
        "    if t[-1] < 30.0:\n",
        "        ratio_30   = np.nan\n",
        "        ratio_30_3 = np.nan\n",
        "    else:\n",
        "        f30_idx = np.searchsorted(t, 30.0, side='right') - 1\n",
        "        ratio_30 = curve[f30_idx] / peak_ct if peak_ct != 0 else np.nan\n",
        "\n",
        "        f3_idx = np.searchsorted(t, 3.0, side='right') - 1\n",
        "        if 0 <= f3_idx < n and curve[f3_idx] != 0:\n",
        "            ratio_30_3 = curve[f30_idx] / curve[f3_idx]\n",
        "        else:\n",
        "            ratio_30_3 = np.nan\n",
        "\n",
        "    def _fix(x): return -1 if np.isnan(x) else x\n",
        "    return [\n",
        "        _fix(ttp), _fix(bh), _fix(dh),\n",
        "        _fix(ratio_30), _fix(ratio_30_3)\n",
        "    ]\n",
        "\n",
        "def extract_quantitative_features(\n",
        "    curve,\n",
        "    time_vector: np.ndarray,\n",
        "    diuretic_time: float = 20,\n",
        "    feature_set: str = \"all\",\n",
        "):\n",
        "    if hasattr(curve, \"cpu\"):\n",
        "        curve = curve.cpu().detach().numpy()\n",
        "\n",
        "    related_work = related_work_features(curve, time_vector, diuretic_time)\n",
        "    domain_expertise = domain_expertise_features(curve, time_vector, diuretic_time)\n",
        "\n",
        "    if feature_set == \"related_work\":\n",
        "        return related_work\n",
        "    elif feature_set == \"domain_expertise\":\n",
        "        return domain_expertise\n",
        "    elif feature_set == \"all\":\n",
        "        return related_work + domain_expertise\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown feature_set {feature_set!r}\")\n",
        "\n",
        "\n",
        "def extract_features_matrix(\n",
        "    X, diuretic_times, patient_ids, time_vectors, feature_set=\"all\"\n",
        "):\n",
        "    feats = []\n",
        "    for curve, dt, pid, t_vec in zip(X, diuretic_times, patient_ids, time_vectors):\n",
        "        feats.append(extract_quantitative_features(\n",
        "            curve, t_vec, dt, feature_set\n",
        "        ))\n",
        "\n",
        "    feats = np.array(feats)\n",
        "\n",
        "    if feature_set == \"domain_expertise\" or \"all\":\n",
        "        split_vals = compute_split_ratio_test(X, time_vectors, patient_ids)\n",
        "        feats = np.column_stack([feats, split_vals])\n",
        "    return feats\n",
        "\n",
        "\n",
        "\n",
        "def compute_split_ratio_test(\n",
        "    curves: list[np.ndarray],\n",
        "    time_vectors: list[np.ndarray],\n",
        "    patient_ids: list[str]\n",
        ") -> np.ndarray:\n",
        "\n",
        "    uptake_areas = []\n",
        "    for curve, t in zip(curves, time_vectors):\n",
        "        # convert torch tensors to numpy if needed\n",
        "        if isinstance(curve, torch.Tensor):\n",
        "            curve = curve.cpu().detach().numpy()\n",
        "        if isinstance(t, torch.Tensor):\n",
        "            t = t.cpu().detach().numpy()\n",
        "\n",
        "        # per-frame durations in minutes, then to seconds\n",
        "        durations_min = np.diff(t, prepend=0.0)\n",
        "        durations_s   = durations_min * 60.0\n",
        "\n",
        "        # mask frames in uptake window 2–3 min\n",
        "        mask = (t >= 2.0) & (t <= 3.0)\n",
        "        if not np.any(mask):\n",
        "            uptake_areas.append(0.0)\n",
        "        else:\n",
        "            # area = sum(counts * duration)\n",
        "            uptake_areas.append(float(np.sum(curve[mask] * durations_s[mask])))\n",
        "\n",
        "    uptake_areas = np.array(uptake_areas, dtype=float)\n",
        "\n",
        "    # group by patient\n",
        "    groups = defaultdict(list)\n",
        "    for idx, pid in enumerate(patient_ids):\n",
        "        groups[pid].append(idx)\n",
        "\n",
        "    # compute split per group\n",
        "    split = np.zeros_like(uptake_areas)\n",
        "    for idxs in groups.values():\n",
        "        if len(idxs) == 2:\n",
        "            i, j = idxs\n",
        "            tot = uptake_areas[i] + uptake_areas[j]\n",
        "            if tot > 0:\n",
        "                split[i] = uptake_areas[i] / tot\n",
        "                split[j] = uptake_areas[j] / tot\n",
        "            else:\n",
        "                split[i] = split[j] = 0.5\n",
        "        else:\n",
        "            for i in idxs:\n",
        "                split[i] = 0.5\n",
        "    return split\n",
        "\n",
        "\n",
        "def calculate_and_save_evaluation_metrics(cv_results, save_filepath, hyperparams=None):\n",
        "  average_metrics = {metric: np.mean(cv_results[f'test_{metric}']) for metric in scoring.keys()}\n",
        "\n",
        "  fold_metrics = []\n",
        "  n_folds = len(cv_results['test_accuracy'])\n",
        "  for i in range(n_folds):\n",
        "      fold_metric = {metric: cv_results[f'test_{metric}'][i] for metric in scoring.keys()}\n",
        "      fold_metric['fold'] = i + 1\n",
        "      fold_metrics.append(fold_metric)\n",
        "\n",
        "  results = {\n",
        "      'fold_metrics': fold_metrics,\n",
        "      'average_metrics': average_metrics,\n",
        "      'hyperparameters': hyperparams if hyperparams is not None else {}\n",
        "\n",
        "\n",
        "  }\n",
        "\n",
        "  with open(save_filepath, 'w') as f:\n",
        "      json.dump(results, f, indent=4)\n",
        "\n",
        "  print(\"Metrics and hyperparameters saved\")\n",
        "\n",
        "\n",
        "def save_fold_metrics_pytorch(fold_metrics, save_filepath, hyperparams=None):\n",
        "    average_metrics = {}\n",
        "    keys = fold_metrics[0].keys()\n",
        "    for key in keys:\n",
        "        values = [fold[key] for fold in fold_metrics]\n",
        "        average_metrics[key] = {\n",
        "            \"mean\": float(np.mean(values)),\n",
        "            \"std\": float(np.std(values))\n",
        "        }\n",
        "\n",
        "    results = {\n",
        "        \"fold_metrics\": fold_metrics,\n",
        "        \"average_metrics\": average_metrics,\n",
        "        \"hyperparameters\": hyperparams if hyperparams is not None else {}\n",
        "    }\n",
        "\n",
        "    with open(save_filepath, 'w') as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "    print(\"Metrics and hyperparameters saved to\", save_filepath)\n",
        "\n",
        "\n",
        "def save_training_data_and_model(model_save_path, training_data_save_path, best_model, X):\n",
        "  dump(best_model, model_save_path)\n",
        "  np.save(training_data_save_path, X)\n",
        "\n",
        "  print(f\"Model saved successfully at {model_save_path}\")\n",
        "  print(f\"Training data saved sucessfully at {training_data_save_path}\")\n",
        "\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    sequences, labels, diuretic_times, patient_folders, time_vectors = zip(*batch)\n",
        "    sequences = [torch.tensor(seq) if not torch.is_tensor(seq) else seq for seq in sequences]\n",
        "\n",
        "    labels = torch.tensor(labels)\n",
        "    diuretic_times = torch.tensor(diuretic_times)\n",
        "\n",
        "    return sequences, labels, diuretic_times, patient_folders, time_vectors\n",
        "\n",
        "def plot_renogram(curve):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.plot(curve, linewidth=1, color='black')\n",
        "    plt.title('Original Renogram Curve')\n",
        "    plt.xlabel('Frame')\n",
        "    plt.ylabel('Counts/sec')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "#For MLP and CNN\n",
        "def aggregate_metrics(fold_metrics):\n",
        "    agg_metrics = {}\n",
        "    for key in fold_metrics[0].keys():\n",
        "        values = [fold[key] for fold in fold_metrics]\n",
        "        agg_metrics[key] = (np.mean(values), np.std(values))\n",
        "    return agg_metrics\n",
        "\n",
        "\n",
        "def fnr_score(y_true, y_pred):\n",
        "    return 1 - recall_score(y_true, y_pred)\n",
        "\n",
        "def specificity_score(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "\n",
        "def fpr_score(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
        "\n",
        "\n",
        "def plot_renograms_patient(X, y, patient_folders, time_vectors):\n",
        "    from collections import defaultdict\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    curves_by_patient = defaultdict(list)\n",
        "    for seq, label, pf, t_vec in zip(X, y, patient_folders, time_vectors):\n",
        "        curves_by_patient[pf].append((seq, label, t_vec))\n",
        "\n",
        "    for pf, curves in curves_by_patient.items():\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        for seq, label, t_vec in curves:\n",
        "            arr = seq.cpu().numpy() if hasattr(seq, \"cpu\") else np.array(seq)\n",
        "            side = \"Left\" if label == 0 else \"Right\"\n",
        "            plt.plot(t_vec, arr, label=side)\n",
        "        plt.title(f\"Diuretic‐phase renogram for patient {pf}\")\n",
        "        plt.xlabel(\"Time since injection (min)\")\n",
        "        plt.ylabel(\"Activity\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def cubic_smooth_curves(curves, time_vectors):\n",
        "  smoothed_curves = []\n",
        "  for seq, t in zip(curves, time_vectors):\n",
        "      seq_np = seq.numpy() if torch.is_tensor(seq) else np.array(seq, dtype=float)\n",
        "\n",
        "      s = 0.005 * len(t) * np.var(seq_np)\n",
        "      spline = UnivariateSpline(t, seq_np, s=s)\n",
        "\n",
        "      smooth_seq = spline(t)\n",
        "      smoothed_curves.append(torch.tensor(smooth_seq, dtype=torch.float32))\n",
        "\n",
        "  return smoothed_curves\n",
        "\n",
        "\n",
        "def plot_raw_vs_smoothed_curves(\n",
        "    X_raw, X_smooth,\n",
        "    time_vectors, patient_folders, diuretic_times,\n",
        "    out_dir,\n",
        "    num_to_plot=140\n",
        "):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    n = min(num_to_plot, len(X_raw))\n",
        "    for i in range(n):\n",
        "        t       = time_vectors[i]\n",
        "        raw     = X_raw[i]\n",
        "        smooth  = X_smooth[i]\n",
        "        pid     = patient_folders[i]\n",
        "        t0      = diuretic_times[i]\n",
        "\n",
        "\n",
        "        if torch.is_tensor(raw):\n",
        "            raw = raw.numpy()\n",
        "        if torch.is_tensor(smooth):\n",
        "            smooth = smooth.numpy()\n",
        "\n",
        "        # plot\n",
        "        fig, ax = plt.subplots(figsize=(8,4))\n",
        "        ax.plot(t, raw,    '-', lw=1, alpha=0.6, label='Raw (noisy)')\n",
        "        ax.plot(t, smooth, '-', lw=3,           label='Smoothed')\n",
        "\n",
        "        if t0 is not None:\n",
        "            ax.axvline(x=t0, color='red', linestyle='--', lw=1.5, label='Diuretic administration')\n",
        "        ax.set_xlabel('Time (min)')\n",
        "        ax.set_ylabel('Counts/Sec')\n",
        "        ax.legend()\n",
        "\n",
        "        # save and close\n",
        "        fname = f\"patient_{pid:}_sample_{i:03d}.png\"\n",
        "        fig.savefig(os.path.join(out_dir, fname), dpi=150, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xLC5YADqXcf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **UNET - Segmentation**"
      ],
      "metadata": {
        "id": "lUf3B6xMkfGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create composite images**"
      ],
      "metadata": {
        "id": "bgLXZUzorn5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patient_folder_pattern = re.compile(r'^[A-Za-z0-9]+$')\n",
        "\n",
        "patient_folders = [f for f in os.listdir(parent_dir_uto)\n",
        "                   if os.path.isdir(os.path.join(parent_dir_uto, f)) and patient_folder_pattern.match(f)]\n",
        "\n",
        "file_pattern = r'^[A-Za-z0-9]+\\.dcm$'\n",
        "\n",
        "for patient_folder in sorted(patient_folders):\n",
        "    patient_folder_path = os.path.join(parent_dir_uto, patient_folder)\n",
        "    print(f\"\\nProcessing patient folder: {patient_folder}\")\n",
        "    composite_image = create_uptake_composite(patient_folder_path, file_pattern, patient_folder)"
      ],
      "metadata": {
        "id": "Mkljm79Wrqe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training and validation**"
      ],
      "metadata": {
        "id": "1ggqmRWRXkM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "images, left_masks, right_masks = load_composite_images_with_roi_labels(\n",
        "    composite_images_dir_uto, kidney_rois_uto_dir\n",
        ")\n",
        "\n",
        "batch_size = 8\n",
        "epochs = 150\n",
        "n_splits = 5\n",
        "\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "all_folds_results = {}\n",
        "\n",
        "best_dice_score = 0.0\n",
        "best_model_path = \"/content/drive/My Drive/uto/segmentation_models/best_unet_model.pth\"\n",
        "best_metrics_path = \"/content/drive/My Drive/uto/segmentation_models/best_unet_metrics.json\"\n",
        "\n",
        "fold_no = 1\n",
        "\n",
        "for train_index, val_index in kf.split(images):\n",
        "    print(f\"\\n=== Training fold {fold_no}/{n_splits} ===\")\n",
        "\n",
        "    X_train, X_val = images[train_index], images[val_index]\n",
        "    left_y_train, left_y_val = left_masks[train_index], left_masks[val_index]\n",
        "    right_y_train, right_y_val = right_masks[train_index], right_masks[val_index]\n",
        "\n",
        "    train_dataset = SegmentationDataset(X_train, left_y_train, right_y_train)\n",
        "    val_dataset = SegmentationDataset(X_val, left_y_val, right_y_val)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n",
        "    )\n",
        "\n",
        "    # Initialize U-Net, optimizer, loss, and metrics\n",
        "    model, optimizer, criterion, metric_funcs = pretrained_unet_model(\n",
        "        backbone=\"resnet34\", lr=1e-4\n",
        "    )\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for images_batch, left_masks_batch, right_masks_batch in train_loader:\n",
        "            images_batch = images_batch.to(device)\n",
        "            left_masks_batch = left_masks_batch.to(device)\n",
        "            right_masks_batch = right_masks_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images_batch)\n",
        "\n",
        "            left_loss = criterion(\n",
        "                outputs[:, 0, :, :].contiguous(),\n",
        "                left_masks_batch.float().contiguous(),\n",
        "            )\n",
        "            right_loss = criterion(\n",
        "                outputs[:, 1, :, :].contiguous(),\n",
        "                right_masks_batch.float().contiguous(),\n",
        "            )\n",
        "            loss = (left_loss + right_loss) / 2.0\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * images_batch.size(0)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        total_samples = 0\n",
        "\n",
        "        cumulative_tp_left = []\n",
        "        cumulative_fp_left = []\n",
        "        cumulative_fn_left = []\n",
        "        cumulative_tn_left = []\n",
        "\n",
        "        cumulative_tp_right = []\n",
        "        cumulative_fp_right = []\n",
        "        cumulative_fn_right = []\n",
        "        cumulative_tn_right = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images_batch, left_masks_batch, right_masks_batch in val_loader:\n",
        "                images_batch = images_batch.to(device)\n",
        "                left_masks_batch = left_masks_batch.to(device)\n",
        "                right_masks_batch = right_masks_batch.to(device)\n",
        "\n",
        "                outputs = model(images_batch)\n",
        "\n",
        "                # Compute loss\n",
        "                left_loss = criterion(\n",
        "                    outputs[:, 0, :, :].contiguous(),\n",
        "                    left_masks_batch.float().contiguous(),\n",
        "                )\n",
        "                right_loss = criterion(\n",
        "                    outputs[:, 1, :, :].contiguous(),\n",
        "                    right_masks_batch.float().contiguous(),\n",
        "                )\n",
        "                loss = (left_loss + right_loss) / 2.0\n",
        "                val_loss += loss.item() * images_batch.size(0)\n",
        "\n",
        "                # Compute binary stats for IoU & Dice\n",
        "                tp_left, fp_left, fn_left, tn_left = smp.metrics.get_stats(\n",
        "                    outputs[:, 0, :, :].unsqueeze(1),\n",
        "                    left_masks_batch,\n",
        "                    mode=\"binary\",\n",
        "                    threshold=0.5,\n",
        "                )\n",
        "                tp_right, fp_right, fn_right, tn_right = smp.metrics.get_stats(\n",
        "                    outputs[:, 1, :, :].unsqueeze(1),\n",
        "                    right_masks_batch,\n",
        "                    mode=\"binary\",\n",
        "                    threshold=0.5,\n",
        "                )\n",
        "\n",
        "                cumulative_tp_left.append(tp_left)\n",
        "                cumulative_fp_left.append(fp_left)\n",
        "                cumulative_fn_left.append(fn_left)\n",
        "                cumulative_tn_left.append(tn_left)\n",
        "\n",
        "                cumulative_tp_right.append(tp_right)\n",
        "                cumulative_fp_right.append(fp_right)\n",
        "                cumulative_fn_right.append(fn_right)\n",
        "                cumulative_tn_right.append(tn_right)\n",
        "\n",
        "                total_samples += images_batch.size(0)\n",
        "\n",
        "\n",
        "        tp_left = torch.cat(cumulative_tp_left)\n",
        "        fp_left = torch.cat(cumulative_fp_left)\n",
        "        fn_left = torch.cat(cumulative_fn_left)\n",
        "        tn_left = torch.cat(cumulative_tn_left)\n",
        "\n",
        "        tp_right = torch.cat(cumulative_tp_right)\n",
        "        fp_right = torch.cat(cumulative_fp_right)\n",
        "        fn_right = torch.cat(cumulative_fn_right)\n",
        "        tn_right = torch.cat(cumulative_tn_right)\n",
        "\n",
        "        iou_metric = metric_funcs[\"iou_score\"]\n",
        "        f1_metric = metric_funcs[\"fscore\"]\n",
        "\n",
        "        iou_left = iou_metric(tp_left, fp_left, fn_left, tn_left, reduction=\"micro\").item()\n",
        "        iou_right = iou_metric(tp_right, fp_right, fn_right, tn_right, reduction=\"micro\").item()\n",
        "\n",
        "        dice_left = f1_metric(tp_left, fp_left, fn_left, tn_left, reduction=\"micro\").item()\n",
        "        dice_right = f1_metric(tp_right, fp_right, fn_right, tn_right, reduction=\"micro\").item()\n",
        "\n",
        "        mean_iou = (iou_left + iou_right) / 2.0\n",
        "        mean_dice = (dice_left + dice_right) / 2.0\n",
        "\n",
        "        val_loss /= total_samples\n",
        "\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{epochs}] \"\n",
        "            f\"Train Loss: {train_loss:.4f} \"\n",
        "            f\"Val Loss: {val_loss:.4f} \"\n",
        "            f\"IoU: {mean_iou:.4f} \"\n",
        "            f\"Dice: {mean_dice:.4f}\"\n",
        "        )\n",
        "\n",
        "    all_folds_results[fold_no] = {\n",
        "        \"train_loss\": train_loss,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"iou_score\": mean_iou,\n",
        "        \"dice_score\": mean_dice,\n",
        "    }\n",
        "\n",
        "    if mean_dice > best_dice_score:\n",
        "        best_dice_score = mean_dice\n",
        "\n",
        "        os.makedirs(os.path.dirname(best_model_path), exist_ok=True)\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "        best_metrics = {\n",
        "            \"fold\": fold_no,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"iou_score\": mean_iou,\n",
        "            \"dice_score\": mean_dice,\n",
        "        }\n",
        "        with open(best_metrics_path, \"w\") as f:\n",
        "            json.dump(best_metrics, f, indent=4)\n",
        "\n",
        "        print(f\">>> New best model found on fold {fold_no} (Dice: {mean_dice:.4f}).\")\n",
        "        print(f\">>> Saved best model to: {best_model_path}\")\n",
        "        print(f\">>> Saved best metrics to: {best_metrics_path}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Fold {fold_no} Dice {mean_dice:.4f} did not exceed best Dice {best_dice_score:.4f}.\")\n",
        "\n",
        "    fold_no += 1\n",
        "\n",
        "total_dice = 0.0\n",
        "total_iou = 0.0\n",
        "\n",
        "for f in range(1, n_splits + 1):\n",
        "    total_dice += all_folds_results[f][\"dice_score\"]\n",
        "    total_iou += all_folds_results[f][\"iou_score\"]\n",
        "\n",
        "mean_dice_over_folds = total_dice / n_splits\n",
        "mean_iou_over_folds = total_iou / n_splits\n",
        "\n",
        "print(\"\\n===== CROSS‐VALIDATION SUMMARY =====\")\n",
        "print(f\"Mean Dice over {n_splits} folds: {mean_dice_over_folds:.4f}\")\n",
        "print(f\"Mean IoU  over {n_splits} folds: {mean_iou_over_folds:.4f}\")\n",
        "\n",
        "# Optionally, you could also dump all_folds_results to disk:\n",
        "all_results_path = \"/content/drive/My Drive/uto/segmentation_models/all_folds_results.json\"\n",
        "with open(all_results_path, \"w\") as f:\n",
        "    json.dump(all_folds_results, f, indent=4)\n",
        "print(f\"Saved all folds’ results to: {all_results_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "t6j72IRFXlKN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualize predictions**"
      ],
      "metadata": {
        "id": "KhHVZ93uBjh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the trained model for a specific fold\n",
        "fold_no = 1\n",
        "model_path = f\"/content/drive/My Drive/segmentation_models_test/fold_{fold_no}_pretrained_unet_model.pth\"\n",
        "model = torch.load(model_path, map_location=device)  # Load the entire model\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load composite image for drsprg_001\n",
        "patient_number = \"001\"\n",
        "img_height=128\n",
        "img_width=128\n",
        "\n",
        "composite_image_path =f\"/content/drive/My Drive/composite_images/composite_image_drsprg_{patient_number}.png\"\n",
        "\n",
        "composite_image = cv2.imread(composite_image_path)\n",
        "composite_image = cv2.resize(composite_image, (img_width, img_height))\n",
        "composite_image = composite_image.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "\n",
        "# Preprocess image for the model\n",
        "image_tensor = torch.tensor(composite_image).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
        "\n",
        "print(\"Image Tensor:\", image_tensor.shape, image_tensor.min().item(), image_tensor.max().item())  # Debug\n",
        "\n",
        "\n",
        "# Get prediction\n",
        "with torch.no_grad():\n",
        "  pred_masks = model(image_tensor)\n",
        "\n",
        "  print(\"Raw Predictions (Min, Max):\", pred_masks.min().item(), pred_masks.max().item())  # Debug\n",
        "\n",
        "  pred_left_mask = (pred_masks[:, 0, :, :] > 0.5).cpu().numpy().squeeze()\n",
        "  pred_right_mask = (pred_masks[:, 1, :, :] > 0.5).cpu().numpy().squeeze()\n",
        "\n",
        "# Visualize the results\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Original composite image\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.imshow(composite_image)\n",
        "plt.title('Composite Image')\n",
        "plt.axis('off')\n",
        "\n",
        "# Predicted left mask\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.imshow(pred_left_mask, cmap='gray')\n",
        "plt.title('Predicted Left Mask')\n",
        "plt.axis('off')\n",
        "\n",
        "# Predicted right mask\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.imshow(pred_right_mask, cmap='gray')\n",
        "plt.title('Predicted Right Mask')\n",
        "plt.axis('off')\n",
        "\n",
        "# Combined prediction for visualization (Optional)\n",
        "plt.subplot(2, 3, 6)\n",
        "combined_pred = np.maximum(pred_left_mask, pred_right_mask)  # Combine left and right masks\n",
        "plt.imshow(combined_pred, cmap='gray')\n",
        "plt.title('Combined Predicted Mask')\n",
        "plt.axis('off')\n",
        "\n",
        "# Visualize raw probabilities for debugging\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.imshow(pred_masks[0, 0, :, :].cpu().numpy(), cmap='hot')\n",
        "plt.title('Raw Predictions (Left)')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GqO0OJqhBmLD",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset analysis**"
      ],
      "metadata": {
        "id": "Hc_pDrTzjj6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Side of obstruction - patient level**"
      ],
      "metadata": {
        "id": "RZNXtFCmnpcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize_obstruction(row):\n",
        "    left  = bool(row[\"left_kidney_label\"])\n",
        "    right = bool(row[\"right_kidney_label\"])\n",
        "    if left and right:\n",
        "        return \"Bilateral\"\n",
        "    elif left:\n",
        "        return \"Left sided\"\n",
        "    elif right:\n",
        "        return \"Right sided\"\n",
        "    else:\n",
        "        return \"None\"\n",
        "\n",
        "# Read labels and compute “obstruction_side”\n",
        "labels_path = os.path.join(patient_labels_uto_dir, \"patient_labels_uto.csv\")\n",
        "labels_df = pd.read_csv(labels_path, sep=\",\")\n",
        "labels_df[\"obstruction_side\"] = labels_df.apply(categorize_obstruction, axis=1)\n",
        "\n",
        "# Get counts in a fixed order\n",
        "order = [\"None\", \"Left sided\", \"Right sided\", \"Bilateral\"]\n",
        "counts = labels_df[\"obstruction_side\"].value_counts().reindex(order, fill_value=0)\n",
        "\n",
        "# Bar plot\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "ax.bar(counts.index, counts.values)\n",
        "ax.set_xlabel(\"Obstruction Side\")\n",
        "ax.set_ylabel(\"Number of Patients\")\n",
        "plt.xticks(rotation=15, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# Plot\n",
        "fname = \"side_of_obstruction_patients_bar.png\"\n",
        "fig.savefig(os.path.join(dataset_analysis_plots_dir, fname), dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "plt.close(fig)\n"
      ],
      "metadata": {
        "id": "Yuf1dorjnx8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Obtruction distribution - kidney level**"
      ],
      "metadata": {
        "id": "V7fke8r0wd94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kidney_labels = labels_df[[\"left_kidney_label\", \"right_kidney_label\"]].values.ravel()\n",
        "unique, counts = np.unique(kidney_labels, return_counts=True)\n",
        "count_map = dict(zip(unique, counts))\n",
        "sizes = [count_map.get(1, 0), count_map.get(0, 0)]\n",
        "labels = [\"Obstructed\", \"Normal\"]\n",
        "\n",
        "total = sum(sizes)\n",
        "\n",
        "def make_autopct(sizes):\n",
        "    def my_autopct(pct):\n",
        "        absolute = int(np.round(pct/100.*total))\n",
        "        return f\"{pct:.1f}%\\n({absolute})\"\n",
        "    return my_autopct\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4,4))\n",
        "wedges, texts, autotexts = ax.pie(\n",
        "    sizes,\n",
        "    labels=labels,\n",
        "    autopct=make_autopct(sizes),\n",
        "    startangle=90,\n",
        "    wedgeprops=dict(width=0.4, edgecolor=\"w\"),\n",
        "    pctdistance=0.75\n",
        ")\n",
        "\n",
        "centre_circle = plt.Circle((0, 0), 0.40, fc=\"white\", linewidth=0)\n",
        "ax.add_artist(centre_circle)\n",
        "\n",
        "ax.axis(\"equal\")\n",
        "plt.setp(autotexts, size=10, weight=\"bold\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# save & show\n",
        "fname_donut = \"obstructed_vs_non_obstructed_kidneys_donut_with_counts.png\"\n",
        "fig.savefig(os.path.join(dataset_analysis_plots_dir, fname_donut), dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "id": "yrM9C8fcGi6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Diuretic Injection Time Spread**"
      ],
      "metadata": {
        "id": "fPcBp2fDsbMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Violin‐plot of the same distribution using fig, ax\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "\n",
        "parts = ax.violinplot(\n",
        "    labels_df['diuretic_time'],\n",
        "    showmeans=True,\n",
        "    showextrema=True,\n",
        "    vert=False)\n",
        "\n",
        "ax.set_xlabel('Diuretic Injection Time (min)')\n",
        "ax.set_yticks([])\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "#Plot\n",
        "out_fname = \"diuretic_variation_horizontal.png\"\n",
        "out_path = os.path.join(dataset_analysis_plots_dir, out_fname)\n",
        "plt.show()\n",
        "plt.close(fig)\n",
        "\n"
      ],
      "metadata": {
        "id": "Q1-buozAsh6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Aquasition analysis - frame count and frame intervals**"
      ],
      "metadata": {
        "id": "1v_4_bj79aeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_2_min_summed_images(patient_folders):\n",
        "\n",
        "  # — 2-min Summed Image for One Example Patient —\n",
        "  example = patient_folders[0]  # or pick a specific ID\n",
        "  example_path = os.path.join(parent_dir_uto, example)\n",
        "  files_ex = sorted([\n",
        "      fn for fn in os.listdir(example_path)\n",
        "      if re.match(file_pattern, fn)\n",
        "  ])\n",
        "\n",
        "  # load first file to get timing & data shape\n",
        "  ds_first = pydicom.dcmread(os.path.join(example_path, files_ex[0]))\n",
        "  if hasattr(ds_first, \"NumberOfFrames\"):\n",
        "      arr = ds_first.pixel_array  # shape = (n_frames, H, W)\n",
        "      # extract frame_time_s exactly as above\n",
        "      if hasattr(ds_first, \"FrameTime\"):\n",
        "          ft_ms = float(ds_first.FrameTime)\n",
        "      else:\n",
        "          ft_ms = float(ds_first.PhaseInformationSequence[0].ActualFrameDuration)\n",
        "      frame_time_s = ft_ms / 1000.0\n",
        "  else:\n",
        "      # stack single-frame files\n",
        "      arr = np.stack([\n",
        "          pydicom.dcmread(os.path.join(example_path, fn)).pixel_array\n",
        "          for fn in files_ex\n",
        "      ])\n",
        "      # assume same timing in first file\n",
        "      if hasattr(ds_first, \"FrameTime\"):\n",
        "          frame_time_s = float(ds_first.FrameTime) / 1000.0\n",
        "      else:\n",
        "          frame_time_s = float(ds_first.PhaseInformationSequence[0].ActualFrameDuration) / 1000.0\n",
        "\n",
        "  # how many frames correspond to 2 minutes?\n",
        "  n_sum = int(np.floor(180.0 / frame_time_s))\n",
        "  total_frames = arr.shape[0]\n",
        "  n_windows = total_frames // n_sum\n",
        "\n",
        "  for i in range(n_windows):\n",
        "      start_idx = i * n_sum\n",
        "      end_idx   = start_idx + n_sum\n",
        "\n",
        "      # sum this 2-min block\n",
        "      sum_img = arr[start_idx:end_idx].sum(axis=0)\n",
        "\n",
        "      # convert to time labels\n",
        "      t_start = start_idx * frame_time_s\n",
        "      t_end   = end_idx   * frame_time_s\n",
        "\n",
        "      # plot & save\n",
        "      fig, ax = plt.subplots(figsize=(4,4))\n",
        "      ax.imshow(sum_img, cmap='gray')\n",
        "      ax.axis('off')\n",
        "      plt.tight_layout()\n",
        "\n",
        "      fname = f\"{example}_sum_{int(t_start)}-{int(t_end)}s.png\"\n",
        "      outpath = os.path.join(dataset_analysis_plots_dir, fname)\n",
        "      fig.savefig(outpath, dpi=150, bbox_inches='tight')\n",
        "      plt.close(fig)\n",
        "\n",
        "  print(f\"Generated {n_windows} two-minute summed images for patient {example}\")\n",
        "\n",
        "def compute_frame_count_and_frame_interval(dcm_file_path, frame_counts, frame_intervals):\n",
        "    ds = pydicom.dcmread(dcm_file_path)\n",
        "\n",
        "    # if the vendor has given us PhaseInformationSequence → variable intervals\n",
        "    if hasattr(ds, \"PhaseInformationSequence\"):\n",
        "        total_frames = 0\n",
        "        for phase in ds.PhaseInformationSequence:\n",
        "            # how many frames were in this phase?\n",
        "            n_phase = int(phase.NumberOfFramesInPhase)\n",
        "            total_frames += n_phase\n",
        "\n",
        "            # actual duration for each of those frames, in ms\n",
        "            ft_ms = float(phase.ActualFrameDuration)\n",
        "            ft_s = ft_ms / 1000.0\n",
        "\n",
        "            # record that interval once per frame\n",
        "            frame_intervals.extend([ft_s] * n_phase)\n",
        "\n",
        "        # record the exam’s total frame count\n",
        "        frame_counts.append(total_frames)\n",
        "\n",
        "    else:\n",
        "        # fallback: constant interval for all frames\n",
        "        # determine number of frames\n",
        "        if hasattr(ds, \"NumberOfFrames\"):\n",
        "            total_frames = int(ds.NumberOfFrames)\n",
        "        else:\n",
        "            raise ValueError(f\"No PhaseInformationSequence or NumberOfFrames in {dcm_file_path}\")\n",
        "\n",
        "        # read the single, constant FrameTime\n",
        "        if hasattr(ds, \"FrameTime\"):\n",
        "            ft_ms = float(ds.FrameTime)\n",
        "        else:\n",
        "            ft_ms = float(ds.PhaseInformationSequence[0].ActualFrameDuration)\n",
        "        ft_s = ft_ms / 1000.0\n",
        "\n",
        "        # append\n",
        "        frame_counts.append(total_frames)\n",
        "        frame_intervals.extend([ft_s] * total_frames)\n",
        "\n",
        "\n",
        "def plot_acquisition_variation(frame_counts, frame_intervals, out_dir):\n",
        "    # —— 1) Histogram of frame counts ——\n",
        "    fig1, ax1 = plt.subplots(figsize=(6,4))\n",
        "    ax1.hist(frame_counts, bins='auto', edgecolor='k')\n",
        "    ax1.set_xlabel('Number of frames per exam')\n",
        "    ax1.set_ylabel('Number of exams')\n",
        "    fig1.tight_layout()\n",
        "    fname1 = 'frame_count_distribution.png'\n",
        "    fig1.savefig(os.path.join(out_dir, fname1), dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig1)\n",
        "    print(f\"Saved frame‐count plot to {os.path.join(out_dir, fname1)}\")\n",
        "\n",
        "    # —— 2) Histogram of frame intervals ——\n",
        "    fig2, ax2 = plt.subplots(figsize=(6,4))\n",
        "    ax2.hist(frame_intervals, bins='auto', edgecolor='k')\n",
        "    ax2.set_xlabel('Frame interval (s)')\n",
        "    ax2.set_ylabel('Number of frames')\n",
        "    fig2.tight_layout()\n",
        "    fname2 = 'frame_interval_distribution.png'\n",
        "    fig2.savefig(os.path.join(out_dir, fname2), dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig2)\n",
        "    print(f\"Saved frame‐interval plot to {os.path.join(out_dir, fname2)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "file_pattern = r'^[A-Za-z0-9]+\\.dcm$'\n",
        "patient_folder_pattern = re.compile(r'^[A-Za-z0-9]+$')\n",
        "\n",
        "patient_folders = [\n",
        "    folder for folder in os.listdir(parent_dir_uto)\n",
        "    if os.path.isdir(os.path.join(parent_dir_uto, folder)) and patient_folder_pattern.match(folder)\n",
        "]\n",
        "\n",
        "#print(f\"Found {len(patient_folders)} patient folders: {patient_folders}\")\n",
        "\n",
        "all_activities = {}\n",
        "\n",
        "frame_counts = []\n",
        "frame_intervals = []\n",
        "\n",
        "\n",
        "for patient_folder in patient_folders:\n",
        "  print(f\"\\nProcessing patient: {patient_folder}\")\n",
        "\n",
        "  patient_folder_path = os.path.join(parent_dir_uto, patient_folder)\n",
        "\n",
        "  print(\"patient_folder_path\", patient_folder_path)\n",
        "\n",
        "  dicom_files = [file for file in os.listdir(patient_folder_path) if re.match(file_pattern, file)]\n",
        "\n",
        "  if not dicom_files:\n",
        "    print(f\"No DICOM files found in {patient_folder_path} matching {file_pattern}.\")\n",
        "\n",
        "  left_activities, right_activities = [], []\n",
        "  time_s_list = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for dcm_file in dicom_files:\n",
        "    dcm_file_path = os.path.join(patient_folder_path, dcm_file)\n",
        "    print(f\"Processing DICOM file: {dcm_file}\")\n",
        "\n",
        "    # Read the DICOM file\n",
        "    #ds = pydicom.dcmread(dcm_file_path)\n",
        "\n",
        "    #save_2_min_summed_images(patient_folders)\n",
        "    compute_frame_count_and_frame_interval(dcm_file_path, frame_counts, frame_intervals)\n",
        "\n",
        "print(\"frame counts\", frame_counts)\n",
        "print(\"frame intervals\", frame_intervals)\n",
        "\n",
        "\n",
        "plot_acquisition_variation(frame_counts, frame_intervals, dataset_analysis_plots_dir )\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F356rM_Ejrnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plot 3-min summed frames**"
      ],
      "metadata": {
        "id": "d-2kbwTG743p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "\n",
        "\n",
        "pattern = os.path.join(dataset_analysis_plots_dir, 'EEEFC908_sum_*s.png')\n",
        "files = glob.glob(pattern)\n",
        "n_imgs  = len(files)\n",
        "\n",
        "\n",
        "# Extract the start‐time from the filename using regex\n",
        "def get_start(fname):\n",
        "    m = re.search(r'_sum_(\\d+)-', fname)\n",
        "    return int(m.group(1)) if m else 0\n",
        "\n",
        "files = sorted(files, key=get_start)\n",
        "\n",
        "# 2) Make the figure + grid with no padding between axes\n",
        "fig = plt.figure(figsize=(12, 7))\n",
        "grid = ImageGrid(\n",
        "    fig, 111,\n",
        "    nrows_ncols=(3, 5),\n",
        "    axes_pad=(0.0, 0.15),\n",
        "    share_all=True,\n",
        ")\n",
        "\n",
        "# 3) Populate the grid\n",
        "for idx, ax in enumerate(grid):\n",
        "    if idx < n_imgs:\n",
        "        img = plt.imread(files[idx])\n",
        "        ax.imshow(img, cmap='gray')\n",
        "\n",
        "        # hide ticks & spines\n",
        "        ax.axis('off')\n",
        "\n",
        "        # compute & set title, nudged up with pad\n",
        "        t0 = get_start(files[idx]) // 60\n",
        "        t1 = (get_start(files[idx]) + 180) // 60\n",
        "        ax.set_title(f'{t0}–{t1} min', fontsize=8, pad=4)\n",
        "    else:\n",
        "        # remove any extra axes entirely\n",
        "        ax.set_axis_off()\n",
        "\n",
        "# 5) Save with zero border padding\n",
        "out = os.path.join(dataset_analysis_plots_dir, 'EEEFC908_tight_montage.png')\n",
        "plt.savefig(out, dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sE_BeqhK79AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Renogram analysis**"
      ],
      "metadata": {
        "id": "HwOeg7O7lf5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create renograms curves over over all frames or 2 min summed frames**"
      ],
      "metadata": {
        "id": "qMqVXd6JkeVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "file_pattern = r'^[A-Za-z0-9]+\\.dcm$'\n",
        "patient_folder_pattern = re.compile(r'^[A-Za-z0-9]+$')\n",
        "\n",
        "patient_folders = [\n",
        "    folder for folder in os.listdir(parent_dir_uto)\n",
        "    if os.path.isdir(os.path.join(parent_dir_uto, folder)) and patient_folder_pattern.match(folder)\n",
        "]\n",
        "\n",
        "print(f\"Found {len(patient_folders)} patient folders: {patient_folders}\")\n",
        "\n",
        "all_activities = {}\n",
        "\n",
        "for patient_folder in patient_folders:\n",
        "  print(f\"\\nProcessing patient: {patient_folder}\")\n",
        "\n",
        "  patient_folder_path = os.path.join(parent_dir_uto, patient_folder)\n",
        "\n",
        "  #Handling left and right rois seperatly\n",
        "  left_roi_path = os.path.join(kidney_rois_uto_dir, f\"left_kidney_mask_{patient_folder}.png\")\n",
        "  right_roi_path = os.path.join(kidney_rois_uto_dir, f\"right_kidney_mask_{patient_folder}.png\")\n",
        "\n",
        "  if not os.path.exists(left_roi_path) or not os.path.exists(right_roi_path):\n",
        "    print(f\"One or both kidney ROIs not found for {patient_folder}. Skipping.\")\n",
        "    continue\n",
        "\n",
        "  # Load the kidney ROIs\n",
        "  left_kidney_roi = load_image(left_roi_path)\n",
        "  right_kidney_roi = load_image(right_roi_path)\n",
        "\n",
        "  create_renograms_raw(patient_folder_path, file_pattern, patient_folder, left_kidney_roi, right_kidney_roi, all_activities)\n",
        "\n",
        "selected_patients = list(all_activities.keys())[:3]\n",
        "\n",
        "for patient in selected_patients:\n",
        "    activity = all_activities[patient]\n",
        "    time = np.arange(len(activity[\"left\"]))\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(time, activity[\"left\"], label=\"Left Kidney\")\n",
        "    plt.plot(time, activity[\"right\"], label=\"Right Kidney\")\n",
        "\n",
        "    plt.title(f\"Renogram for Patient: {patient}\")\n",
        "    plt.xlabel(\"Time (frames)\")\n",
        "    plt.ylabel(\"Counts/sec\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "#Save renogram dataset as a dictionary to disk\n",
        "output_dataset_path = '/content/drive/MyDrive/uto/activity_curve_datasets/kidney_activity_dataset_2_min_summed_uto.npz'\n",
        "np.savez(output_dataset_path, **all_activities)\n",
        "\n",
        "print(f\"Saved renogram dataset to path {output_dataset_path}\")\n"
      ],
      "metadata": {
        "id": "gC5LWk3XklSb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model training**"
      ],
      "metadata": {
        "id": "fI7zS684mZKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prepare training data**"
      ],
      "metadata": {
        "id": "l_bIqzXFAEw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "training_mode = \"feature\"\n",
        "feature_set=\"all\"\n",
        "curve_phase=\"whole\"\n",
        "\n",
        "TARGET_LEN_WHOLE   = 220\n",
        "TARGET_LEN_DIURETIC = 120\n",
        "TARGET_LEN_2_MIN_SUMMED = 20\n",
        "\n",
        "\n",
        "smoothed_curves_out_dir = '/content/drive/MyDrive/uto/smoothed_renograms'\n",
        "\n",
        "#Read renogram dataset from disk\n",
        "output_dataset_path = '/content/drive/MyDrive/uto/activity_curve_datasets/kidney_activity_dataset_raw_uto.npz'\n",
        "activities = np.load(output_dataset_path, allow_pickle=True)\n",
        "activities = {key: activities[key].tolist() for key in activities.files}\n",
        "\n",
        "# Read labels from csv (with two labels per patient: one for each kidney)\n",
        "labels_path = os.path.join(patient_labels_uto_dir, 'patient_labels_uto.csv')\n",
        "labels_df = pd.read_csv(labels_path, sep=',')\n",
        "\n",
        "patient_labels = {\n",
        "    row['patient_folder']: (row['left_kidney_label'], row['right_kidney_label'], row['diuretic_time'])\n",
        "    for _, row in labels_df.iterrows()\n",
        "}\n",
        "\n",
        "# Create dataset with one kidney curve per training sample\n",
        "dataset = RenogramDatasetUTO(activities, patient_labels, dicom_dir=parent_dir_uto)\n",
        "\n",
        "data_loader = DataLoader(dataset, batch_size=len(dataset), collate_fn=custom_collate_fn)\n",
        "X_raw, y, diuretic_times, patient_folders, time_vectors = prepare_training_data(data_loader, training_mode)\n",
        "\n",
        "#Cubic smooth to reduce noise\n",
        "X = cubic_smooth_curves(X_raw, time_vectors)\n",
        "\n",
        "orig_time_vectors = list(time_vectors)\n",
        "orig_X_smooth     = [c.clone() if torch.is_tensor(c) else c.copy()\n",
        "                     for c in X]\n",
        "orig_X_raw        = [c.clone() if torch.is_tensor(c) else c.copy()\n",
        "                     for c in X_raw]\n",
        "\n",
        "#plot_raw_vs_smoothed_curves(X_raw, X, time_vectors, patient_folders, diuretic_times, smoothed_curves_out_dir)\n",
        "\n",
        "#plot_renograms_patient(X, y, patient_folders, time_vectors)\n",
        "\n",
        "\n",
        "t_half_dict  = {}\n",
        "y_true_dict  = {}\n",
        "pid_counts   = {}\n",
        "\n",
        "if training_mode == \"datapoint\":\n",
        "  resampled_seqs = []\n",
        "  resampled_tvs  = []\n",
        "  if curve_phase == \"whole\":\n",
        "    # interpolate every full curve to TARGET_LEN_WHOLE\n",
        "    for seq, t_vec in zip(X, time_vectors):\n",
        "        seq_np = seq.numpy() if torch.is_tensor(seq) else np.array(seq, dtype=float)\n",
        "        f = interp1d(t_vec, seq_np, kind='linear', fill_value='extrapolate')\n",
        "\n",
        "        new_t = np.linspace(t_vec[0], t_vec[-1], TARGET_LEN_WHOLE)\n",
        "        new_seq = f(new_t)\n",
        "        resampled_seqs.append(torch.tensor(new_seq, dtype=torch.float32))\n",
        "        resampled_tvs.append(new_t)\n",
        "\n",
        "\n",
        "  if curve_phase == \"diuretic\":\n",
        "    for seq, t_vec, dt in zip(X, time_vectors, diuretic_times):\n",
        "        seq_np = seq.numpy() if torch.is_tensor(seq) else np.array(seq, dtype=float)\n",
        "        # find where diuretic injection starts\n",
        "        start_i = int(np.searchsorted(t_vec, dt))\n",
        "        diu_seq = seq_np[start_i:]\n",
        "        diu_t   = t_vec[start_i:]\n",
        "        # now interpolate that segment\n",
        "        f = interp1d(diu_t, diu_seq, kind='linear', fill_value='extrapolate')\n",
        "        new_t = np.linspace(diu_t[0], diu_t[-1], TARGET_LEN_DIURETIC)\n",
        "        new_seq = f(new_t)\n",
        "        resampled_seqs.append(torch.tensor(new_seq, dtype=torch.float32))\n",
        "        resampled_tvs.append(new_t)\n",
        "\n",
        "  X = torch.stack(resampled_seqs, dim=0)\n",
        "  time_vectors = resampled_tvs\n",
        "\n",
        "if training_mode == \"feature\":\n",
        "  X = extract_features_matrix(X, diuretic_times, patient_folders, time_vectors, feature_set=feature_set)\n",
        "\n",
        "if training_mode == \"baseline\":\n",
        "  for curve, t_vec, dt, pid_base, true_label in zip(X, time_vectors, diuretic_times, patient_folders, y):\n",
        "    th = compute_t_half(curve, t_vec, dt)\n",
        "\n",
        "    count = pid_counts.get(pid_base, 0) + 1\n",
        "    pid_counts[pid_base] = count\n",
        "    if count == 1:\n",
        "        key = f\"{pid_base}_left\"\n",
        "    elif count == 2:\n",
        "        key = f\"{pid_base}_right\"\n",
        "    else:\n",
        "        key = f\"{pid_base}_{count}\"\n",
        "\n",
        "    t_half_dict[key] = th\n",
        "    y_true_dict[key] = int(true_label)\n",
        "\n",
        "\n",
        "  print(f\"Computed T₁/₂ for {len(t_half_dict)} kidneys ({len(pid_counts)} patients).\")\n",
        "\n",
        "  all_keys    = list(t_half_dict.keys())\n",
        "  t_half_list = [t_half_dict[k] for k in all_keys]\n",
        "  y_true_list = [y_true_dict[k] for k in all_keys]\n",
        "\n",
        "  #Replace NaN with a large placeholder\n",
        "  finite_vals = [v for v in t_half_list if not np.isnan(v)]\n",
        "  if finite_vals:\n",
        "      nan_sub = max(finite_vals) + 1.0\n",
        "  else:\n",
        "      nan_sub = 1.0\n",
        "  t_half_scores = [ (v if not np.isnan(v) else nan_sub) for v in t_half_list ]\n",
        "\n",
        "  y_true_arr  = np.array(y_true_list, dtype=int)\n",
        "  t_half_arr  = np.array(t_half_scores, dtype=float)\n",
        "\n",
        "\n",
        "  fpr, tpr, thresh = roc_curve(y_true_arr, t_half_arr, pos_label=1)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(fpr, tpr, lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
        "  plt.plot([0, 1], [0, 1], lw=1, linestyle=\"--\", label=\"Chance level\")\n",
        "  plt.xlabel(\"False Positive Rate\")\n",
        "  plt.ylabel(\"True Positive Rate\")\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "  print(f\"ROC‐AUC (continuous T₁/₂): {roc_auc:.3f}\")\n",
        "\n",
        "  youden        = tpr - fpr\n",
        "  best_idx      = np.argmax(youden)\n",
        "  best_tau      = thresh[best_idx]\n",
        "  best_sens     = tpr[best_idx]\n",
        "  best_spec     = 1.0 - fpr[best_idx]\n",
        "  best_youden   = youden[best_idx]\n",
        "\n",
        "  print(f\"Youden’s J max at τ = {best_tau:.2f} min\")\n",
        "  print(f\"  → Sensitivity = {best_sens:.3f}\")\n",
        "  print(f\"  → Specificity = {best_spec:.3f}\")\n",
        "  print(f\"  → Youden’s J  = {best_youden:.3f}\")\n",
        "\n",
        "  thresholds = list(range(5, 21))\n",
        "\n",
        "  results    = pd.DataFrame(\n",
        "      index=thresholds,\n",
        "      columns=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"fnr\", \"fpr\", \"specificity\"]\n",
        "  )\n",
        "\n",
        "  for τ in thresholds:\n",
        "      y_true_list = []\n",
        "      y_pred_list = []\n",
        "      for key, th in t_half_dict.items():\n",
        "          true_label = y_true_dict[key]\n",
        "\n",
        "          if np.isnan(th) or th > τ:\n",
        "              pred = 1\n",
        "          else:\n",
        "              pred = 0\n",
        "\n",
        "          y_true_list.append(true_label)\n",
        "          y_pred_list.append(pred)\n",
        "\n",
        "      y_true_arr = np.array(y_true_list, dtype=int)\n",
        "      y_pred_arr = np.array(y_pred_list, dtype=int)\n",
        "\n",
        "      acc  = accuracy_score(y_true_arr, y_pred_arr)\n",
        "      prec = precision_score(y_true_arr, y_pred_arr, zero_division=0)\n",
        "      rec  = recall_score(y_true_arr, y_pred_arr, zero_division=0)\n",
        "      f1   = f1_score(y_true_arr, y_pred_arr, zero_division=0)\n",
        "\n",
        "      tn, fp, fn, tp = confusion_matrix(y_true_arr, y_pred_arr, labels=[0,1]).ravel()\n",
        "\n",
        "      fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
        "      fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
        "      spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "\n",
        "      results.loc[τ] = [acc, prec, rec, f1, fnr, fpr, spec]\n",
        "\n",
        "  # Print the sweep table\n",
        "  print(\"\\n=== T₁/₂ Threshold Sweep (pre‐diuretic) ===\")\n",
        "  print(results)\n",
        "\n",
        "  # Which τ gives best F₁?\n",
        "  best_tau = results[\"f1\"].astype(float).idxmax()\n",
        "  best_f1  = results.loc[best_tau, \"f1\"]\n",
        "  print(f\"\\nBest F₁ is {best_f1:.3f} at τ = {best_tau} minutes.\")\n",
        "\n",
        "\n",
        "  print(f\"=== Full classification report at τ = {best_tau} min ===\")\n",
        "  y_true_best = []\n",
        "  y_pred_best = []\n",
        "\n",
        "  for key, th in t_half_dict.items():\n",
        "      true_label = y_true_dict[key]\n",
        "      if np.isnan(th) or (th >= best_tau):\n",
        "          pred = 1\n",
        "      else:\n",
        "          pred = 0\n",
        "      y_true_best.append(true_label)\n",
        "      y_pred_best.append(pred)\n",
        "\n",
        "  y_true_best = np.array(y_true_best, dtype=int)\n",
        "  y_pred_best = np.array(y_pred_best, dtype=int)\n",
        "\n",
        "  print(\"Accuracy :\", accuracy_score(y_true_best, y_pred_best))\n",
        "  print(\"Precision:\", precision_score(y_true_best, y_pred_best, zero_division=0))\n",
        "  print(\"Recall   :\", recall_score(y_true_best, y_pred_best, zero_division=0))\n",
        "  print(\"F1 Score :\", f1_score(y_true_best, y_pred_best, zero_division=0))\n",
        "\n",
        "  # Confusion matrix & derived metrics\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true_best, y_pred_best, labels=[0,1]).ravel()\n",
        "  fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
        "  fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
        "  spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "\n",
        "  print(f\"FNR       : {fnr:.3f}\")\n",
        "  print(f\"FPR       : {fpr:.3f}\")\n",
        "  print(f\"Specificity: {spec:.3f}\\n\")\n",
        "\n",
        "  print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "  print(confusion_matrix(y_true_best, y_pred_best))\n",
        "  print(\"\\nFull classification report:\")\n",
        "  print(classification_report(y_true_best, y_pred_best, zero_division=0))\n",
        "\n",
        "\n",
        "\n",
        "if training_mode == \"datapoint\" or training_mode == \"feature\":\n",
        "  for i in range(10):\n",
        "    print(f\"Row {i}:\")\n",
        "    print(\" Curve length\", len(X[i]))\n",
        "    print(\"  Kidney curve:\", X[i])\n",
        "    print(\"  Label:\", y[i])\n",
        "    print(\"  Patient:\", patient_folders[i])\n",
        "    print(\"  Diuretic time:\", diuretic_times[i])\n",
        "\n",
        "  fnr = make_scorer(fnr_score)\n",
        "  specificity = make_scorer(specificity_score)\n",
        "  fpr = make_scorer(fpr_score)\n",
        "\n",
        "  scoring = {\n",
        "      'accuracy':  'accuracy',\n",
        "      'precision': 'precision',\n",
        "      'f1':        'f1',\n",
        "      'roc_auc':   'roc_auc',\n",
        "      'recall':    'recall',\n",
        "      'fnr':       fnr,\n",
        "      'fpr':       fpr,\n",
        "      'specificity': specificity\n",
        "      }\n"
      ],
      "metadata": {
        "id": "_Vdp_DVoAMQ4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Support vector machine**"
      ],
      "metadata": {
        "id": "rayKlaxUJYNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Build a pipeline: scaler + SVM\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC(probability=True, random_state=SEED, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "# 2) Parameter grid: prefix with 'svm__'\n",
        "param_grid = {\n",
        "    'svm__C':      [0.1, 1, 10, 100, 1000],\n",
        "    'svm__kernel': ['linear', 'rbf', 'poly'],\n",
        "    'svm__gamma':  ['scale', 'auto'],\n",
        "}\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "grid_search_svm = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=skf,\n",
        "    scoring='accuracy',      # or you could pick 'f1' or any single metric\n",
        "    verbose=2,\n",
        "    n_jobs=-1,\n",
        "    return_train_score=False,\n",
        ")\n",
        "\n",
        "grid_search_svm.fit(X, y)\n",
        "\n",
        "print(\"Best params:\", grid_search_svm.best_params_)\n",
        "best_svm_model = grid_search_svm.best_estimator_\n",
        "\n",
        "# 3) Get all your CV metrics at once\n",
        "cv_results_svm = cross_validate(\n",
        "    best_svm_model,\n",
        "    X, y,\n",
        "    cv=skf,\n",
        "    scoring=scoring,\n",
        "    return_train_score=False\n",
        ")\n",
        "\n",
        "for metric in scoring:\n",
        "    scores = cv_results_svm[f'test_{metric}']\n",
        "    print(f\"{metric:9s} → mean: {scores.mean():.3f}  std: {scores.std():.3f}\")\n",
        "\n",
        "\n",
        "training_data_svm_save_path = f\"/content/drive/MyDrive/uto/models/svm_models/datapoints/svm_best_training_data.npy\"\n",
        "scaler_save_path = os.path.join(svm_model_dir, \"scaler_raw.joblib\")\n",
        "svm_model_save_path_total = os.path.join(svm_model_dir, f\"svm_best_model_raw.joblib\")\n",
        "\n",
        "#dump(best_svm_model, svm_model_save_path_total)\n",
        "#np.save(training_data_svm_save_path, X)\n",
        "#dump(scaler, scaler_save_path)\n",
        "\n",
        "calculate_and_save_evaluation_metrics(cv_results_svm, evaluation_metrics_results_svm, grid_search_svm.best_params_)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "geGWd0EaJaNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Decision tree**"
      ],
      "metadata": {
        "id": "qvthBNKOIe7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier(class_weight='balanced', random_state=SEED)\n",
        "\n",
        "# 3) Hyperparameter grid (no prefix needed if not in a Pipeline)\n",
        "param_grid_dt = {\n",
        "    \"max_depth\": [3, 5, 7],  # Reduce complexity\n",
        "    \"min_samples_split\": [2, 5],  # Fewer options\n",
        "    \"min_samples_leaf\": [1, 2]  # Smaller range\n",
        "}\n",
        "\n",
        "# 4) 5‑fold splitter\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "# 5) GridSearchCV on the full dataset\n",
        "grid_search_dt = GridSearchCV(\n",
        "    dt,\n",
        "    param_grid_dt,\n",
        "    cv=skf,\n",
        "    scoring='accuracy',   # you can choose any one metric to optimize\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    return_train_score=False\n",
        ")\n",
        "grid_search_dt.fit(X, y)\n",
        "\n",
        "print(\"Best DT params:\", grid_search_dt.best_params_)\n",
        "best_dt_model = grid_search_dt.best_estimator_\n",
        "\n",
        "# 6) Cross‑validate the best tree for all metrics\n",
        "cv_results_dt = cross_validate(\n",
        "    best_dt_model,\n",
        "    X, y,\n",
        "    cv=skf,\n",
        "    scoring=scoring,\n",
        "    return_train_score=False\n",
        "\n",
        ")\n",
        "\n",
        "# 7) Print per‑metric means ± stds\n",
        "for metric in scoring:\n",
        "    scores = cv_results_dt[f'test_{metric}']\n",
        "    print(f\"{metric:9s} → mean: {scores.mean():.3f}  std: {scores.std():.3f}\")\n",
        "\n",
        "\n",
        "model_save_path = f\"/content/drive/MyDrive/uto/models/dt_models/dt_model_best.joblib\"\n",
        "training_data_save_path = f\"/content/drive/MyDrive/uto/models/dt_models/dt_training_data_best.npy\"\n",
        "\n",
        "save_training_data_and_model(model_save_path, training_data_save_path, best_dt_model, X)\n",
        "calculate_and_save_evaluation_metrics(cv_results_dt, evaluation_metrics_results_dt, grid_search_dt.best_params_)\n",
        "\n"
      ],
      "metadata": {
        "id": "k6pdf0XpInFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **XGBOOST**"
      ],
      "metadata": {
        "id": "YTWOPZFhK9mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Compute scale_pos_weight from y\n",
        "counter = Counter(y)\n",
        "n_neg = counter[0]\n",
        "n_pos = counter[1]\n",
        "scale_pos_weight = n_neg / n_pos\n",
        "\n",
        "# 3) Define XGBClassifier\n",
        "xgb = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    random_state=SEED,\n",
        "    use_label_encoder=False,   # newer XGBoost versions\n",
        "    eval_metric='logloss',     # shuts off warning\n",
        "    scale_pos_weight=scale_pos_weight\n",
        ")\n",
        "\n",
        "# 4) Hyperparameter grid\n",
        "param_grid_xgb = {\n",
        "    'max_depth':        [3, 4, 5, 6],\n",
        "    'learning_rate':    [0.01, 0.05, 0.1, 0.2],\n",
        "    'n_estimators':     [50, 100, 200],\n",
        "    'subsample':        [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "# 5) 5‑fold splitter\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "# 6) GridSearchCV on full X, y\n",
        "grid_search_xgb = GridSearchCV(\n",
        "    xgb,\n",
        "    param_grid=param_grid_xgb,\n",
        "    cv=skf,\n",
        "    scoring='accuracy',    # pick your main metric here\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    return_train_score=False\n",
        ")\n",
        "grid_search_xgb.fit(X, y)\n",
        "\n",
        "print(\"Best XGB params:\", grid_search_xgb.best_params_)\n",
        "best_xgb_model = grid_search_xgb.best_estimator_\n",
        "\n",
        "# 7) Cross‑validate best model for all metrics\n",
        "cv_results_xgb = cross_validate(\n",
        "    best_xgb_model,\n",
        "    X, y,\n",
        "    cv=skf,\n",
        "    scoring=scoring,\n",
        "    return_train_score=False\n",
        ")\n",
        "\n",
        "# 8) Summarize\n",
        "for metric in scoring:\n",
        "    scores = cv_results_xgb[f'test_{metric}']\n",
        "    print(f\"{metric:9s} → mean: {scores.mean():.3f}  std: {scores.std():.3f}\")\n",
        "\n",
        "model_save_path = f\"/content/drive/MyDrive/uto/models/xgboost_models/xgboost_best.joblib\"\n",
        "training_data_save_path = f\"/content/drive/MyDrive/uto/models/xgboost_models/xgboost_training_data_best.npy\"\n",
        "\n",
        "save_training_data_and_model(model_save_path, training_data_save_path, best_xgb_model, X)\n",
        "calculate_and_save_evaluation_metrics(cv_results_xgb, evaluation_metrics_results_xgboost, grid_search_xgb.best_params_)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wMDipOZqLVRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Random Forest**"
      ],
      "metadata": {
        "id": "oksp7z7jNjK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(class_weight=\"balanced\", random_state=SEED)\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid_rf = {\n",
        "    \"n_estimators\": [50, 100, 200],  # Number of trees in the forest\n",
        "    \"max_depth\": [None, 3, 5, 10, 15],  # Tree depth\n",
        "    \"min_samples_split\": [2, 5, 10],  # Minimum samples for splitting a node\n",
        "    \"min_samples_leaf\": [1, 2, 5],  # Minimum samples for a leaf node\n",
        "    \"max_features\": [\"sqrt\", \"log2\"],  # Number of features per split\n",
        "    \"bootstrap\": [True, False]  # Whether to use bootstrap sampling\n",
        "}\n",
        "\n",
        "\n",
        "# Perform Grid Search with Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "# 6) GridSearchCV on full X, y\n",
        "grid_search_rf = GridSearchCV(\n",
        "    rf,\n",
        "    param_grid=param_grid_rf,\n",
        "    cv=skf,\n",
        "    scoring='accuracy',    # pick your main metric here\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    return_train_score=False\n",
        ")\n",
        "grid_search_rf.fit(X, y)\n",
        "\n",
        "best_params = grid_search_rf.best_params_\n",
        "best_rf    = grid_search_rf.best_estimator_\n",
        "\n",
        "fold_metrics = []\n",
        "all_metrics  = {'accuracy':[], 'precision':[], 'recall':[], 'f1':[], 'roc_auc':[]}\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), start=1):\n",
        "    # re-instantiate & train on this fold\n",
        "    model = clone(best_rf).set_params(**best_params)\n",
        "    model.fit(X[train_idx], y[train_idx])\n",
        "\n",
        "    # get true & predicted\n",
        "    y_true  = y[test_idx]\n",
        "    y_pred  = model.predict(X[test_idx])\n",
        "    y_score = model.predict_proba(X[test_idx])[:,1]\n",
        "\n",
        "    # compute standard metrics\n",
        "    acc   = accuracy_score(y_true, y_pred)\n",
        "    prec  = precision_score(y_true, y_pred)\n",
        "    rec   = recall_score(y_true, y_pred)\n",
        "    f1    = f1_score(y_true, y_pred)\n",
        "    roc   = auc(*roc_curve(y_true, y_score)[:2])\n",
        "\n",
        "    # collect for averaging\n",
        "    for name,val in zip(['accuracy','precision','recall','f1','roc_auc'],\n",
        "                        [acc,prec,rec,f1,roc, y]):\n",
        "        all_metrics[name].append(val)\n",
        "\n",
        "    # store per‐fold detail + raw curves\n",
        "    fpr, tpr, thr = roc_curve(y_true, y_score)\n",
        "    fold_metrics.append({\n",
        "        'fold':       fold,\n",
        "        'accuracy':   acc,\n",
        "        'precision':  prec,\n",
        "        'recall':     rec,\n",
        "        'f1':         f1,\n",
        "        'roc_auc':    roc,\n",
        "        'fpr':        fpr.tolist(),\n",
        "        'tpr':        tpr.tolist(),\n",
        "        'thresholds': thr.tolist(),\n",
        "        'y_true':     y_true.tolist(),\n",
        "        'y_score':    y_score.tolist()\n",
        "    })\n",
        "\n",
        "# compute averages\n",
        "average_metrics = { name: float(np.mean(vals)) for name,vals in all_metrics.items() }\n",
        "\n",
        "# package everything\n",
        "results = {\n",
        "    'fold_metrics':    fold_metrics,\n",
        "    'average_metrics': average_metrics,\n",
        "    'hyperparameters': { **best_params, 'voting': 'n/a (single model)' }\n",
        "}\n",
        "\n",
        "model_save_path = f\"/content/drive/MyDrive/uto/models/rf_models/rf_model_best.joblib\"\n",
        "training_data_save_path = f\"/content/drive/MyDrive/uto/models/rf_models/rf_training_data_best.npy\"\n",
        "\n",
        "save_training_data_and_model(model_save_path, training_data_save_path, best_rf, X)\n",
        "\n",
        "\n",
        "# save to your drive\n",
        "out_path = \"/content/drive/MyDrive/uto/models/rf_models/rf_evaluation.json\"\n",
        "with open(out_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7fhZ5GljNn3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **KNN**"
      ],
      "metadata": {
        "id": "as9MPtH7K0qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Build a Pipeline (scale → KNN)\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn',    KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# 3) Hyperparameter grid\n",
        "param_grid = {\n",
        "    'knn__n_neighbors': [3, 5, 10, 15],\n",
        "    'knn__weights':    ['uniform', 'distance']\n",
        "}\n",
        "\n",
        "# 4) 5-fold splitter\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "# 5) GridSearchCV on the *entire* dataset\n",
        "grid_search_knn = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=skf,\n",
        "    scoring='accuracy',   # or whichever single metric you want to optimize\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    return_train_score=False\n",
        ")\n",
        "grid_search_knn.fit(X, y)\n",
        "\n",
        "best_params = grid_search_knn.best_params_\n",
        "best_knn    = grid_search_knn.best_estimator_\n",
        "\n",
        "fold_metrics = []\n",
        "all_metrics  = {'accuracy':[], 'precision':[], 'recall':[], 'f1':[], 'roc_auc':[]}\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), start=1):\n",
        "    # re-instantiate & train on this fold\n",
        "    model = clone(best_knn).set_params(**best_params)\n",
        "    model.fit(X[train_idx], y[train_idx])\n",
        "\n",
        "    # get true & predicted\n",
        "    y_true  = y[test_idx]\n",
        "    y_pred  = model.predict(X[test_idx])\n",
        "    y_score = model.predict_proba(X[test_idx])[:,1]\n",
        "\n",
        "    # compute standard metrics\n",
        "    acc   = accuracy_score(y_true, y_pred)\n",
        "    prec  = precision_score(y_true, y_pred)\n",
        "    rec   = recall_score(y_true, y_pred)\n",
        "    f1    = f1_score(y_true, y_pred)\n",
        "    roc   = auc(*roc_curve(y_true, y_score)[:2])\n",
        "\n",
        "    # collect for averaging\n",
        "    for name,val in zip(['accuracy','precision','recall','f1','roc_auc'],\n",
        "                        [acc,prec,rec,f1,roc]):\n",
        "        all_metrics[name].append(val)\n",
        "\n",
        "    # store per‐fold detail + raw curves\n",
        "    fpr, tpr, thr = roc_curve(y_true, y_score)\n",
        "    fold_metrics.append({\n",
        "        'fold':       fold,\n",
        "        'accuracy':   acc,\n",
        "        'precision':  prec,\n",
        "        'recall':     rec,\n",
        "        'f1':         f1,\n",
        "        'roc_auc':    roc,\n",
        "        'fpr':        fpr.tolist(),\n",
        "        'tpr':        tpr.tolist(),\n",
        "        'thresholds': thr.tolist(),\n",
        "        'y_true':     y_true.tolist(),\n",
        "        'y_score':    y_score.tolist()\n",
        "    })\n",
        "\n",
        "# compute averages\n",
        "average_metrics = { name: float(np.mean(vals)) for name,vals in all_metrics.items() }\n",
        "\n",
        "# package everything\n",
        "results = {\n",
        "    'fold_metrics':    fold_metrics,\n",
        "    'average_metrics': average_metrics,\n",
        "    'hyperparameters': { **best_params, 'voting': 'n/a (single model)' }\n",
        "}\n",
        "\n",
        "\n",
        "model_save_path = f\"/content/drive/MyDrive/uto/models/knn_models/knn_model_best_summed.joblib\"\n",
        "training_data_save_path = f\"/content/drive/MyDrive/uto/models/knn_models/knn_training_data_summed_best.npy\"\n",
        "\n",
        "save_training_data_and_model(model_save_path, training_data_save_path, best_knn, X)\n",
        "\n",
        "# save to your drive\n",
        "out_path = \"/content/drive/MyDrive/uto/models/knn_models/knn_evaluation_summed.json\"\n",
        "with open(out_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "\n",
        "np.save(\"/content/drive/MyDrive/uto/models/knn_models/knn_training_data_timevecs.npy\", np.vstack(resampled_tvs))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qu96OULNK4WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **MLP**"
      ],
      "metadata": {
        "id": "YbQc6CmdMi4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, dropout_rate=0.3):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "        self.output = nn.Linear(32, 2)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def train_mlp(model, train_loader, test_loader, epochs=100, lr=0.001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        train_acc = correct / total\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "    print(\"\\nTraining Complete!\")\n",
        "\n",
        "\n",
        "def evaluate_mlp(model, test_loader):\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    all_pred_probs = []  # For ROC-AUC\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            # Compute probabilities using softmax\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            # For binary classification, use the probability of the positive class\n",
        "            all_pred_probs.extend(probabilities[:, 1].cpu().numpy())\n",
        "\n",
        "            # Get predicted labels using argmax\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_pred_probs = np.array(all_pred_probs)\n",
        "\n",
        "    # Compute metrics using sklearn functions\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds)\n",
        "    recall = recall_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    roc_auc = roc_auc_score(all_labels, all_pred_probs)\n",
        "\n",
        "    # Compute confusion matrix and derive FNR: FNR = FN / (FN + TP)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    # Make sure we have a binary confusion matrix (assume cm = [[TN, FP], [FN, TP]])\n",
        "    if cm.size == 4:\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "    else:\n",
        "        tn, fp, fn, tp = 0, 0, 0, 0  # In case something unexpected happens\n",
        "\n",
        "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "    # Optionally, print out the metrics\n",
        "    print(\"\\nTest Metrics:\")\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n",
        "    print(f\"ROC AUC:   {roc_auc:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(f\"FNR:       {fnr:.4f}\")\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'f1': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'recall': recall,\n",
        "        'fnr': fnr,\n",
        "        'fpr': fpr,\n",
        "        'specificity': tnr\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "X_np = np.array(X)\n",
        "y_np = np.array(y)\n",
        "\n",
        "# Hyperparameter grids\n",
        "batch_sizes = [16, 32]\n",
        "dropouts    = [0.2, 0.3]\n",
        "lrs         = [1e-3, 5e-4]\n",
        "\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "best_avg_acc = 0\n",
        "best_params   = None\n",
        "epochs = 100\n",
        "\n",
        "\n",
        "for batch_size, dropout, lr in product(batch_sizes, dropouts, lrs):\n",
        "    print(f\"\\nTesting combination: batch_size={batch_size}, dropout={dropout}, lr={lr}\")\n",
        "    fold_metrics = []\n",
        "\n",
        "    # Five-fold cross validation loop\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
        "\n",
        "        # reproducibility\n",
        "        torch.manual_seed(SEED)\n",
        "        np.random.seed(SEED)\n",
        "        random.seed(SEED)\n",
        "\n",
        "        # split data\n",
        "        X_tr, y_tr = X[train_idx], y[train_idx]\n",
        "        X_va, y_va = X[val_idx], y[val_idx]\n",
        "\n",
        "        # to tensors and loaders\n",
        "        train_ds = TensorDataset(torch.tensor(X_tr, dtype=torch.float32),\n",
        "                                 torch.tensor(y_tr, dtype=torch.long))\n",
        "        val_ds   = TensorDataset(torch.tensor(X_va, dtype=torch.float32),\n",
        "                                 torch.tensor(y_va, dtype=torch.long))\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "        val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # init model with dropout\n",
        "        model = MLP(input_dim=X.shape[1], dropout_rate=dropout)\n",
        "        train_mlp(model, train_loader, val_loader, epochs=epochs, lr=lr)\n",
        "        metrics = evaluate_mlp(model, val_loader)\n",
        "        fold_metrics.append(metrics)\n",
        "\n",
        "    # average accuracy over folds\n",
        "    fold_accuracies = [m['accuracy'] for m in fold_metrics]\n",
        "    avg_acc = np.mean(fold_accuracies)\n",
        "    print(f\"Avg CV accuracy: {avg_acc:.4f}\")\n",
        "\n",
        "    if avg_acc > best_avg_acc:\n",
        "        best_avg_acc = avg_acc\n",
        "        best_params  = {'batch_size': batch_size,\n",
        "                        'dropout':    dropout,\n",
        "                        'lr':         lr}\n",
        "\n",
        "\n",
        "save_filepath = \"/content/drive/MyDrive/uto/models/mlp_models/evaluation_metrics.json\"\n",
        "\n",
        "save_fold_metrics_pytorch(fold_metrics, save_filepath, best_params)\n",
        "\n"
      ],
      "metadata": {
        "id": "LIshxrP3MmeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CNN**"
      ],
      "metadata": {
        "id": "Q0te8kKvkGne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Simple1DCNN(nn.Module):\n",
        "    def __init__(self, input_length, num_classes, dropout=0.5):\n",
        "        super(Simple1DCNN, self).__init__()\n",
        "        # 1D Convolutional layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "\n",
        "        # Pooling layer\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # Use the hyperparameter for dropout\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Calculate the final length after 3 pooling layers\n",
        "        self.final_length = input_length // 8\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(128 * self.final_length, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Expected input shape: [batch_size, 1, input_length]\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def initModel(X, y, lr=0.001, dropout=0.5):\n",
        "    num_classes = len(set(y))\n",
        "    input_length = X.shape[1] if len(X.shape) == 2 else X.shape[2]\n",
        "\n",
        "    model = Simple1DCNN(input_length=input_length, num_classes=num_classes, dropout=dropout)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.01, patience=3)\n",
        "\n",
        "    return model, criterion, optimizer, scheduler, device\n",
        "\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for volumes, labels in dataloader:\n",
        "        # Move data to GPU\n",
        "        volumes, labels = volumes.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(volumes)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    print(f\"Training Loss: {epoch_loss:.4f}\")\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    y_true = []\n",
        "    y_pred_probs = []  # For ROC-AUC\n",
        "    y_pred_labels = [] # For confusion matrix and accuracy\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for volumes, labels in dataloader:\n",
        "            volumes, labels = volumes.to(device), labels.to(device)\n",
        "            outputs = model(volumes)\n",
        "            loss = criterion(outputs, labels)\n",
        "            # Multiply loss by the batch size for correct averaging later\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "            # Get positive class probabilities\n",
        "            probabilities = torch.softmax(outputs, dim=1)[:, 1]\n",
        "            # Get predicted labels using argmax (consistent with CrossEntropyLoss)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred_probs.extend(probabilities.cpu().numpy())\n",
        "            y_pred_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Calculate the average loss over the entire dataset\n",
        "    avg_loss = total_loss / len(dataloader.dataset)\n",
        "    accuracy = 100 * np.mean(np.array(y_true) == np.array(y_pred_labels))\n",
        "\n",
        "    # Compute confusion matrix values\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_labels).ravel()\n",
        "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # Sensitivity / Recall\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # Specificity\n",
        "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    f1_score = (2 * precision * tpr) / (precision + tpr) if (precision + tpr) > 0 else 0\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
        "\n",
        "\n",
        "    metrics = {\n",
        "        \"loss\": avg_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'f1': f1_score,\n",
        "        'roc_auc': roc_auc,\n",
        "        'recall': tpr,\n",
        "        'fnr': fnr,\n",
        "        'fpr': fpr,\n",
        "        'specificity': tnr\n",
        "    }\n",
        "\n",
        "    print(f\"Evaluation Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "cblitbnHkJaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Right before your grid‐search:\n",
        "cnn_param_grid = {\n",
        "    'num_epochs': [10, 20],\n",
        "    'batch_size': [8, 16],\n",
        "    'lr': [0.001, 0.0005],\n",
        "    'dropout': [0.5, 0.3]\n",
        "}\n",
        "\n",
        "X_np = np.array(X)\n",
        "y_np = np.array(y)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_params   = None\n",
        "results       = []\n",
        "\n",
        "for num_epochs, batch_size, lr, dropout in itertools.product(\n",
        "    cnn_param_grid['num_epochs'],\n",
        "    cnn_param_grid['batch_size'],\n",
        "    cnn_param_grid['lr'],\n",
        "    cnn_param_grid['dropout']\n",
        "):\n",
        "    print(f\"\\nCombo: epochs={num_epochs}, batch={batch_size}, lr={lr}, drop={dropout}\")\n",
        "    fold_metrics = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_np, y_np), 1):\n",
        "        # reproducibility\n",
        "        torch.manual_seed(SEED)\n",
        "        np.random.seed(SEED)\n",
        "        random.seed(SEED)\n",
        "\n",
        "        # split\n",
        "        X_tr, y_tr = X_np[train_idx], y_np[train_idx]\n",
        "        X_va, y_va = X_np[val_idx], y_np[val_idx]\n",
        "\n",
        "        # datasets + loaders (drop_last so BatchNorm never sees batch=1)\n",
        "        train_ds = TensorDataset(torch.tensor(X_tr).unsqueeze(1).float(),\n",
        "                                 torch.tensor(y_tr).long())\n",
        "        val_ds   = TensorDataset(torch.tensor(X_va).unsqueeze(1).float(),\n",
        "                                 torch.tensor(y_va).long())\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size,\n",
        "                                  shuffle=True,  drop_last=True)\n",
        "        val_loader   = DataLoader(val_ds,   batch_size=batch_size,\n",
        "                                  shuffle=False, drop_last=False)\n",
        "\n",
        "        # init model for this fold\n",
        "        model, criterion, optimizer, scheduler, device = initModel(\n",
        "            X_tr, y_tr, lr=lr, dropout=dropout\n",
        "        )\n",
        "        # reweight classes\n",
        "        cw = compute_class_weight(\"balanced\",\n",
        "                                  classes=np.unique(y_tr), y=y_tr)\n",
        "        criterion.weight = torch.tensor(cw, device=device).float()\n",
        "\n",
        "        # train for num_epochs\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "            scheduler.step(train_loss)\n",
        "\n",
        "        # evaluate & collect\n",
        "        metrics = evaluate(model, val_loader, criterion, device)\n",
        "        fold_metrics.append(metrics)\n",
        "\n",
        "    # summarize this combo\n",
        "    avg_val_loss = np.mean([m['loss'] for m in fold_metrics])\n",
        "    print(f\"→ Avg val loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    results.append({\n",
        "      'num_epochs': num_epochs,\n",
        "      'batch_size': batch_size,\n",
        "      'lr': lr,\n",
        "      'dropout': dropout,\n",
        "      'avg_val_loss': avg_val_loss\n",
        "    })\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_params   = {\n",
        "          'num_epochs': num_epochs,\n",
        "          'batch_size': batch_size,\n",
        "          'lr': lr,\n",
        "          'dropout': dropout\n",
        "        }\n",
        "\n",
        "print(\"\\nBest hyperparameters:\", best_params)\n",
        "\n",
        "final_metrics = aggregate_metrics(fold_metrics)\n",
        "save_filepath = \"/content/drive/MyDrive/uto/models/cnn_models/evaluation_metrics.json\"\n",
        "\n",
        "save_fold_metrics_pytorch(fold_metrics, save_filepath, best_params)\n"
      ],
      "metadata": {
        "id": "-Tcc-53mmZ7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HbjkA1SAThf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stacking Ensemble**"
      ],
      "metadata": {
        "id": "gyKGj1atWMCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import load\n",
        "from sklearn.preprocessing  import StandardScaler\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_training_data(best_knn_path, best_rf_path):\n",
        "  datapoint_data = np.load(best_knn_path.parent / 'knn_training_data_best.npy')\n",
        "  feature_data  = np.load(best_rf_path.parent  / 'rf_training_data_best.npy')\n",
        "\n",
        "  print(\"datapoint_data shape:\", datapoint_data.shape)\n",
        "  print(\"feature_data shape:\", feature_data.shape)\n",
        "\n",
        "  # 2) Check they have the same number of rows\n",
        "  assert datapoint_data.shape[0] == feature_data.shape[0], \"Mismatch in number of samples!\"\n",
        "\n",
        "  # 3) Concatenate horizontally\n",
        "  X_full = np.hstack([datapoint_data, feature_data])\n",
        "  print(\"X_full shape:\", X_full.shape)\n",
        "\n",
        "  return datapoint_data, feature_data, X_full\n",
        "\n",
        "def select_curve(X):\n",
        "    return X[:, :n_curve]\n",
        "\n",
        "def select_feat(X):\n",
        "    return X[:, n_curve:]\n",
        "\n",
        "\n",
        "\n",
        "# 1) Point to your base “models” folder\n",
        "BASE = Path(\"/content/drive/MyDrive/uto/models\")\n",
        "\n",
        "# 2) Build each path by joining the sub‐folders and file name\n",
        "knn_path = BASE / \"knn_models\" / \"datapoints\" / \"whole_curve\" / \"knn_best.joblib\"\n",
        "rf_path  = BASE / \"rf_models\"  / \"features\"   / \"combined\"     / \"rf_best.joblib\"\n",
        "ens_path  = Path(\"/content/drive/MyDrive/uto\") / \"ensamble_models\"  / \"ens_best_model.joblib\"\n",
        "ens_stack_path  = Path(\"/content/drive/MyDrive/uto\") / \"ensamble_models\"  / \"stack_best_model.joblib\"\n",
        "\n",
        "\n",
        "\n",
        "# 3) (Optional) sanity‐check\n",
        "assert knn_path.exists(), f\"KnN model not found at {knn_path}\"\n",
        "assert rf_path.exists(),  f\"RF  model not found at {rf_path}\"\n",
        "assert ens_path.exists(),  f\"ENS  model not found at {ens_path}\"\n",
        "\n",
        "\n",
        "# 4) Load\n",
        "knn_model = load(knn_path)\n",
        "rf_model  = load(rf_path)\n",
        "ens_model = load(ens_path)\n",
        "ens_stack_model = load(ens_stack_path)\n",
        "\n",
        "knn_data_path = knn_path.parent / \"knn_training_data_best.npy\"\n",
        "rf_data_path  = rf_path.parent  / \"rf_training_data_best.npy\"\n",
        "knn_timevecs_path = knn_path.parent / \"knn_training_data_timevecs.npy\"\n",
        "\n",
        "datapoint_data = np.load(knn_data_path)\n",
        "feature_data   = np.load(rf_data_path)\n",
        "\n",
        "X_full = np.hstack([datapoint_data, feature_data])\n",
        "\n",
        "n_curve = datapoint_data.shape[1]\n",
        "\n",
        "\n",
        "datapoint_pipe = Pipeline([\n",
        "    ('select_curve', FunctionTransformer(select_curve, validate=False)),\n",
        "    ('knn_pipe',     knn_model)\n",
        "])\n",
        "\n",
        "feature_pipe = Pipeline([\n",
        "    ('select_feat',  FunctionTransformer(select_feat, validate=False)),\n",
        "    ('scale',        StandardScaler()),\n",
        "    ('clf',          rf_model)\n",
        "])"
      ],
      "metadata": {
        "id": "eqD9cO2rmxCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training session**"
      ],
      "metadata": {
        "id": "XxdLl841hzKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "#!ls -R /content/drive/MyDrive/uto/models/knn_models\n",
        "\n",
        "#ensemble = VotingClassifier(\n",
        "    #estimators=[('raw', knn_model), ('feat', pipeB)],\n",
        "    #voting='soft'\n",
        "#)\n",
        "\n",
        "#param_grid_ens = {\n",
        "  #'voting':   ['soft','hard'],\n",
        "  #'weights':  [(w1,w2) for w1 in (1,2,3) for w2 in (1,2,3)]\n",
        "#}\n",
        "\n",
        "# 4) 5-fold outer splitter (same you’ve been using)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "\n",
        "stack = StackingClassifier(\n",
        "    estimators      = [('raw', datapoint_pipe), ('feat', feature_pipe)],\n",
        "    final_estimator = LogisticRegression(max_iter=1000, random_state=SEED),\n",
        "    cv               = skf,\n",
        "    passthrough     = False    # purely learns weights on the two legs\n",
        ")\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'final_estimator__C':        [0.01, 0.1, 1, 10],\n",
        "    'passthrough':               [False, True]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator      = stack,\n",
        "    param_grid     = param_grid,\n",
        "    cv             = skf,\n",
        "    scoring        = make_scorer(f1_score),\n",
        "    n_jobs         = -1,\n",
        "    verbose        = 2,\n",
        "    return_train_score = False\n",
        ")\n",
        "grid.fit(X_full, y)\n",
        "\n",
        "best_stack = grid.best_estimator_\n",
        "print(\"Grid-search best params:\", grid.best_params_)\n",
        "\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'roc_auc': make_scorer(roc_auc_score, needs_threshold=True)\n",
        "}\n",
        "#cv_res = cross_validate(\n",
        "   # best_stack, X_full, y,\n",
        "   #cv=skf, scoring=scoring,\n",
        "   # return_estimator=False\n",
        "#)\n",
        "\n",
        "y_pred_cv = cross_val_predict(best_stack, X_full, y, cv=skf, method='predict')\n",
        "\n",
        "cm = confusion_matrix(y, y_pred_cv)\n",
        "print(\"Overall 5-fold CV confusion matrix:\\n\", cm)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=best_stack.classes_)\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "disp.plot(ax=ax, cmap=plt.cm.Blues, colorbar=False)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#average_metrics = { metric: float(np.mean(cv_res[f'test_{metric}']))\n",
        "                    #for metric in scoring }\n",
        "#print(\"Average CV metrics:\", average_metrics)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) Inspect the learned weights\n",
        "# ----------------------------------------\n",
        "#coefs = best_stack.final_estimator_.coef_[0]\n",
        "#print(\"Learned soft-voting weights (raw, feat):\", coefs)\n",
        "\n",
        "\n",
        "model_save_path=\"/content/drive/MyDrive/uto/ensamble_models/stack_best_model.joblib\"\n",
        "training_data_save_path=\"/content/drive/MyDrive/uto/ensamble_models/stack_training_data.json\"\n",
        "\n",
        "#save_training_data_and_model(model_save_path, training_data_save_path, best_stack, X_full)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9zyLRNRVWQvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1) Prepare output folder ---\n",
        "out_mis_dir = \"/content/drive/MyDrive/uto/misclassified_plots\"\n",
        "os.makedirs(out_mis_dir, exist_ok=True)\n",
        "\n",
        "# --- 2) Loop over folds, collect misclassified cases ---\n",
        "misclassified = []\n",
        "for fold, (train_idx, test_idx) in enumerate(skf.split(X_full, y), start=1):\n",
        "    # clone the tuned stack and fit on the train split\n",
        "    model = clone(best_stack)\n",
        "    model.fit(X_full[train_idx], y[train_idx])\n",
        "\n",
        "    # get test‐set predictions & scores\n",
        "    X_test   = X_full[test_idx]\n",
        "    y_true   = y[test_idx]\n",
        "    y_score  = model.predict_proba(X_test)[:,1]\n",
        "    y_pred   = (y_score >= 0.5).astype(int)\n",
        "\n",
        "    # record any mismatches\n",
        "    for local_i, (yt, yp) in enumerate(zip(y_true, y_pred)):\n",
        "        if yt != yp:\n",
        "            global_idx = test_idx[local_i]\n",
        "            misclassified.append({\n",
        "                \"fold\": fold,\n",
        "                \"idx\":  int(global_idx),\n",
        "                \"patient_id\": patient_folders[global_idx],\n",
        "                \"true\": yt,\n",
        "                \"pred\": yp\n",
        "            })\n",
        "\n",
        "# --- 3) Print summary ---\n",
        "print(f\"Total misclassified cases: {len(misclassified)}\")\n",
        "print(\"Unique patient IDs:\", sorted({m[\"patient_id\"] for m in misclassified}))\n",
        "\n",
        "# --- 4) Plot the first N misclassified curves ---\n",
        "N = 8\n",
        "for case in misclassified[:N]:\n",
        "    idx      = case[\"idx\"]\n",
        "    pid      = case[\"patient_id\"]\n",
        "    true_lbl = case[\"true\"]\n",
        "    pred_lbl = case[\"pred\"]\n",
        "\n",
        "    # pull the raw & smoothed curves + time vector\n",
        "    raw_curve    = orig_X_raw[idx]\n",
        "    smooth_curve = orig_X_smooth[idx]\n",
        "    t_vec        = orig_time_vectors[idx]\n",
        "\n",
        "    # ensure numpy\n",
        "    raw_curve    = raw_curve.detach().cpu().numpy()    if hasattr(raw_curve, \"detach\") else np.array(raw_curve)\n",
        "    smooth_curve = smooth_curve.detach().cpu().numpy() if hasattr(smooth_curve, \"detach\") else np.array(smooth_curve)\n",
        "    t_vec        = np.array(t_vec)\n",
        "\n",
        "    # make the plot\n",
        "    fig, ax = plt.subplots(figsize=(6,4))\n",
        "    ax.plot(t_vec, raw_curve,    alpha=0.6, label=\"raw\")\n",
        "    ax.plot(t_vec, smooth_curve, linewidth=1.5, label=\"smoothed\")\n",
        "    ax.set_xlabel(\"Time (min)\")\n",
        "    ax.set_ylabel(\"Counts/sec\")\n",
        "    ax.set_title(f\"true={true_lbl}  pred={pred_lbl}\")\n",
        "    ax.legend()\n",
        "\n",
        "    # optionally save:\n",
        "    fname = f\"mis_{pid}_fold{case['fold']}.png\"\n",
        "    fig.savefig(os.path.join(out_mis_dir, fname), dpi=150, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "print(f\"Saved up to {min(N, len(misclassified))} misclassified plots to\\n  {out_mis_dir}\")\n"
      ],
      "metadata": {
        "id": "5Y-VdTy1Eq3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Quantitative analysis**"
      ],
      "metadata": {
        "id": "RHu-jo5bZF0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Performance metric setup**"
      ],
      "metadata": {
        "id": "cBQQzDRp46RF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, glob, json\n",
        "from pathlib import Path\n",
        "from scipy.stats import wilcoxon\n",
        "\n",
        "\n",
        "base_dir = Path('/content/drive/MyDrive/uto/models')\n",
        "\n",
        "records = []\n",
        "for path in base_dir.rglob('*.json'):\n",
        "    text = path.read_text()\n",
        "    data = json.loads(text)\n",
        "    fold_f1s = [fold['f1'] for fold in data.get('fold_metrics', [])]\n",
        "\n",
        "    records.append({\n",
        "        'main_group': data['main_group'],\n",
        "        'subgroup':   data['subgroup'],\n",
        "        'model':      data['model'],\n",
        "        'hyperparams': data['hyperparameters'],\n",
        "        'mean_f1':   data['average_metrics']['f1'],\n",
        "        'fold_f1s': fold_f1s\n",
        "\n",
        "    })\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "print(\"df\", df)"
      ],
      "metadata": {
        "id": "XzSSWXOu5IHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Best models from each group**"
      ],
      "metadata": {
        "id": "-xQzo-xX5q61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick the single best model for each main_group\n",
        "best = ( df\n",
        "         .sort_values('mean_f1', ascending=False)\n",
        "         .groupby('main_group', as_index=False)\n",
        "         .first()\n",
        "       )\n",
        "\n",
        "print(best[['main_group','subgroup','model','mean_f1']])"
      ],
      "metadata": {
        "id": "ayQAuwb45wr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Datapoint subgroups analysis**"
      ],
      "metadata": {
        "id": "7K5W1aDy5WMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_dp = df[df['main_group'] == 'datapoint']\n",
        "\n",
        "df_dp = df[df['main_group']=='datapoint']\n",
        "pivot_dp = df_dp.pivot(index='model', columns='subgroup', values='mean_f1')\n",
        "\n",
        "# make sure columns match exactly:\n",
        "pivot_dp = pivot_dp[['whole_curve','diuretic_phase']]\n",
        "\n",
        "# Summary stats\n",
        "summary = pivot_dp.agg(['mean','std']).T.rename_axis('subgroup')\n",
        "print(summary)\n",
        "\n",
        "# Run the paired Wilcoxon signed-rank test\n",
        "f1_whole = pivot_dp['whole_curve'].values\n",
        "f1_diur  = pivot_dp['diuretic_phase'].values\n",
        "\n",
        "stat, p_val = wilcoxon(f1_whole, f1_diur)\n",
        "print(f\"Wilcoxon signed-rank test on 7 models: W={stat:.1f}, p={p_val:.3f}\")\n"
      ],
      "metadata": {
        "id": "ifmbTdUIZP_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature subgroups analysis**"
      ],
      "metadata": {
        "id": "eWEfDNWb55Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import friedmanchisquare\n",
        "from itertools      import combinations\n",
        "\n",
        "# 1) Pivot your df to get one row per model, columns=subgroups\n",
        "df_feat = df[df['main_group']=='feature']\n",
        "pivot_feat = df_feat.pivot(index='model',\n",
        "                           columns='subgroup',\n",
        "                           values='mean_f1')\n",
        "\n",
        "\n",
        "# make sure the columns are in the right order:\n",
        "pivot_feat = pivot_feat[[\n",
        "    'domain_expertise',\n",
        "    'related_work',\n",
        "    'combined'\n",
        "]]\n",
        "\n",
        "# Summary stats\n",
        "summary = pivot_feat.agg(['mean','std']).T.rename_axis('subgroup')\n",
        "print(summary)\n",
        "\n",
        "# Friedman test across the three feature sets\n",
        "stat_f, p_f = friedmanchisquare(\n",
        "    pivot_feat['domain_expertise'],\n",
        "    pivot_feat['related_work'],\n",
        "    pivot_feat['combined']\n",
        ")\n",
        "print(f\"Friedman χ²={stat_f:.2f}, p={p_f:.3f}\")\n",
        "\n",
        "# If p_f < 0.05, do pairwise Wilcoxon with Bonferroni\n",
        "if p_f < 0.05:\n",
        "    alpha = 0.05\n",
        "    m = 3  # number of pairwise tests\n",
        "    for a, b in combinations(pivot_feat.columns, 2):\n",
        "        stat_w, p_w = wilcoxon(pivot_feat[a], pivot_feat[b])\n",
        "        p_corr = min(p_w * m, 1.0)\n",
        "        print(f\"{a} vs {b}: W={stat_w:.1f}, raw p={p_w:.3f}, Bonferroni p={p_corr:.3f}\")\n",
        "else:\n",
        "    print(\"No overall difference (Friedman p >= 0.05), skip post-hoc.\")"
      ],
      "metadata": {
        "id": "OzWtgxVW5-x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Main group analysis**"
      ],
      "metadata": {
        "id": "xbLH8TB-Dqve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best F1 PER model within each main_group\n",
        "best_dp   = df[df['main_group']=='datapoint'] .groupby('model')['mean_f1'].max()\n",
        "best_feat = df[df['main_group']=='feature']   .groupby('model')['mean_f1'].max()\n",
        "\n",
        "# Align them in one DataFrame\n",
        "compare = pd.concat([best_dp, best_feat], axis=1, keys=['datapoint','feature'])\n",
        "compare = compare.dropna()\n",
        "print(compare)\n",
        "\n",
        "dp_vals   = compare['datapoint'].values\n",
        "feat_vals = compare['feature'].values\n",
        "\n",
        "stat, p_val = wilcoxon(dp_vals, feat_vals)\n",
        "print(f\"Wilcoxon W={stat:.1f}, p={p_val:.3f}\")\n",
        "\n",
        "summary = compare.agg(['mean','std']).T.rename_axis('main_group')\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "73Cf9aXpDy9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Global XAI**"
      ],
      "metadata": {
        "id": "TbYg7vF9C1UK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Best feature model**"
      ],
      "metadata": {
        "id": "NuRbAcYy1uLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "rf_feats = [\n",
        "    \"Mean\", \"Variance\", \"Skewness\", \"Kurtosis\",\n",
        "    \"C_last\", \"Slope_0_5_min\", \"Slope_15_20_min\",\n",
        "    \"Length\", \"Time to peek\", \"Peek to half peak\",\n",
        "    \"Diuretic T1/2\", \"30min/peak\", \"30min/3min\", \"Split Function\"\n",
        "]\n",
        "\n",
        "print(\"rf_feats\", rf_feats)\n",
        "\n",
        "rf_scaler = pipeB.named_steps['scale']\n",
        "rf_clf    = pipeB.named_steps['clf']\n",
        "\n",
        "rf_scaler.fit(feature_data)\n",
        "X_scaled = rf_scaler.transform(feature_data)\n",
        "\n",
        "explainer_rf = shap.TreeExplainer(rf_clf)\n",
        "\n",
        "shap_vals = explainer_rf.shap_values(X_scaled)\n",
        "\n",
        "rf_scaler.fit(feature_data)\n",
        "X_scaled = rf_scaler.transform(feature_data)\n",
        "\n",
        "explainer_rf   = shap.TreeExplainer(rf_clf)\n",
        "\n",
        "shap_vals_list = explainer_rf.shap_values(X_scaled)\n",
        "\n",
        "shap_arr = np.stack(shap_vals_list, axis=0)\n",
        "\n",
        "sv_pos = shap_arr[:, :, 1]\n",
        "\n",
        "print(\"sv_pos shape:\", sv_pos.shape)    # should be (114,13)\n",
        "print(\"X_scaled shape:\", X_scaled.shape) # (114,13)\n",
        "\n",
        "# final bar-chart of global importances\n",
        "shap.summary_plot(\n",
        "    sv_pos,\n",
        "    X_scaled,\n",
        "    feature_names=rf_feats,\n",
        "    max_display=len(rf_feats)\n",
        ")\n"
      ],
      "metadata": {
        "id": "kcHh0bU5C7be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Best datapoint model**"
      ],
      "metadata": {
        "id": "o5bUlQfX16J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import logit\n",
        "\n",
        "time_feats = [f\"time_{i}\" for i in range(n_curve)]\n",
        "\n",
        "curves = datapoint_data\n",
        "\n",
        "bg_idx     = np.random.RandomState(42).choice(len(datapoint_data), 110, replace=False)\n",
        "bg_curves  = datapoint_data[bg_idx]\n",
        "\n",
        "explainer_knn = shap.KernelExplainer(\n",
        "    lambda X: knn_model.predict_proba(X)[:,1],\n",
        "    bg_curves\n",
        ")\n",
        "\n",
        "\n",
        "shap_pos = explainer_knn.shap_values(datapoint_data, nsamples=2000)\n",
        "\n",
        "abs_shap        = np.abs(shap_pos)\n",
        "mean_per_frame  = abs_shap.mean(axis=0)\n",
        "\n",
        "T = np.load(knn_timevecs_path)\n",
        "T = np.asarray(T)\n",
        "mean_times = T.mean(axis=0)\n",
        "\n",
        "BIN_SIZE = 3\n",
        "\n",
        "t_min, t_max = mean_times.min(), mean_times.max()\n",
        "edges = np.arange(t_min, t_max + BIN_SIZE, BIN_SIZE)\n",
        "\n",
        "bin_idx = np.digitize(mean_times, edges, right=False) - 1\n",
        "\n",
        "bin_shap = []\n",
        "for i in range(len(edges)-1):\n",
        "    mask = (bin_idx == i)\n",
        "    if not mask.any():\n",
        "        continue\n",
        "    bin_shap.append({\n",
        "        \"interval_s\":  (edges[i], edges[i+1]),\n",
        "        \"mean_SHAP\":   mean_per_frame[mask].mean(),\n",
        "        \"n_frames\":    int(mask.sum())\n",
        "    })\n",
        "\n",
        "bin_shap.sort(key=lambda x: x[\"mean_SHAP\"], reverse=True)\n",
        "\n",
        "for b in bin_shap:\n",
        "    start, end = b[\"interval_s\"]\n",
        "    print(f\"{start:.0f}–{end:.0f}min: mean|SHAP|={b['mean_SHAP']:.4f} over {b['n_frames']} frames\")\n"
      ],
      "metadata": {
        "id": "24DbX1ld18PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bar plot**"
      ],
      "metadata": {
        "id": "U8tTG63ujbcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract labels & values\n",
        "labels = [\n",
        "    f\"{int(start)}–{int(end)}min\"\n",
        "    for start,end in (b[\"interval_s\"] for b in bin_shap)\n",
        "]\n",
        "\n",
        "global_shap = np.array([ b[\"mean_SHAP\"] for b in bin_shap ])  # shape (n_bins,)\n",
        "\n",
        "shap_vals_2d = global_shap[np.newaxis, :]                       # now shape (1, n_bins)\n",
        "\n",
        "\n",
        "shap.summary_plot(\n",
        "    shap_vals_2d,\n",
        "    feature_names=labels,\n",
        "    plot_type=\"bar\",\n",
        "    max_display=len(labels)   # show all bins\n",
        ")\n",
        "\n",
        "\n",
        "# bar‐plot\n",
        "#shap.summary_plot(\n",
        "   # shap_pos,\n",
        "   # features=values,\n",
        "   # feature_names=labels,\n",
        "   # plot_type=\"bar\",\n",
        "#)\n",
        "\n",
        "\n",
        "#plt.figure()\n",
        "#plt.bar(labels, values)\n",
        "#plt.xticks(rotation=45, ha=\"right\")\n",
        "#plt.ylabel(\"Mean |SHAP|\")\n",
        "#plt.title(\"2-min interval importances\")\n",
        "#plt.tight_layout()\n",
        "#plt.show()\n"
      ],
      "metadata": {
        "id": "5nfHCWMZjf1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bee swarm plot**"
      ],
      "metadata": {
        "id": "hzUFlo4ijg2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_bins = len(edges) - 1\n",
        "n_samples = shap_pos.shape[0]\n",
        "\n",
        "# Labels for each bin\n",
        "bin_labels = [\n",
        "    f\"{int(edges[i])}–{int(edges[i+1])}min\"\n",
        "    for i in range(n_bins)\n",
        "]\n",
        "\n",
        "\n",
        "sample_bin_signed = np.zeros((n_samples, n_bins))\n",
        "for b in range(n_bins):\n",
        "    mask = (bin_idx == b)\n",
        "    if mask.any():\n",
        "        # mean SHAP (with sign) of all frames in bin b, per sample\n",
        "        sample_bin_signed[:, b] = shap_pos[:, mask].sum(axis=1)\n",
        "\n",
        "\n",
        "# 2) Build the corresponding per-sample, per-bin FEATURE matrix:\n",
        "#    here we take the *raw* curve values in each bin and average them\n",
        "sample_bin_feat = np.zeros((n_samples, n_bins))\n",
        "for b in range(n_bins):\n",
        "    mask = (bin_idx == b)\n",
        "    if mask.any():\n",
        "        sample_bin_feat[:, b] = datapoint_data[:, mask].mean(axis=1)\n",
        "\n",
        "# Pass the 2-D array as “shap_values” and a DataFrame (or list) for feature names:\n",
        "shap.summary_plot(\n",
        "    sample_bin_signed,\n",
        "    sample_bin_feat,    # raw feature values (same shape), used for coloring\n",
        "    feature_names=bin_labels,\n",
        "    plot_type=\"dot\"   # the classic beeswarm\n",
        ")"
      ],
      "metadata": {
        "id": "EOaRinSPjkQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dependency plot**"
      ],
      "metadata": {
        "id": "JmguUloL07Zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_bins = [i for i,_ in sorted(\n",
        "    enumerate(bin_shap),\n",
        "    key=lambda x: x[1][\"mean_SHAP\"],\n",
        "    reverse=True\n",
        ")[:3]]\n",
        "print(\"Top bins:\", [(bin_labels[i], i) for i in top_bins])\n",
        "\n",
        "\n",
        "for idx in top_bins:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    shap.dependence_plot(\n",
        "      idx,\n",
        "      sample_bin_signed,\n",
        "      sample_bin_feat,\n",
        "      feature_names=bin_labels,\n",
        "      interaction_index=\"auto\"\n",
        "  )\n",
        "    plt.title(f\"SHAP dependence for {bin_labels[idx]}\")\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "SF5its4G1ccs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ensamble model**"
      ],
      "metadata": {
        "id": "UeKsozJZF-MT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setup**"
      ],
      "metadata": {
        "id": "K2GDLynf1pgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "rf_feats = [\n",
        "    \"Mean\", \"Variance\", \"Skewness\", \"Kurtosis\",\n",
        "    \"C_last\", \"Slope_0_5_min\", \"Slope_15_20_min\",\n",
        "    \"Length\", \"Time to peek\", \"Peek to half peak\",\n",
        "    \"Diuretic T1/2\", \"30min/peak\", \"30min/3min\", \"Split Function\"\n",
        "]\n",
        "\n",
        "bg_idx = np.random.RandomState(SEED).choice(len(X_full), 110, replace=False)\n",
        "explainer_ens = shap.KernelExplainer(\n",
        "    lambda X: ens_stack_model.predict_proba(X)[:,1],\n",
        "    X_full[bg_idx]\n",
        ")\n",
        "shap_vals = explainer_ens.shap_values(X_full, nsamples=2000)\n",
        "\n",
        "\n",
        "n_curve     = datapoint_data.shape[1]\n",
        "shap_curves = shap_vals[:, :n_curve]\n",
        "shap_rf     = shap_vals[:, n_curve:]\n",
        "\n",
        "T = np.load(knn_timevecs_path)\n",
        "mean_times = T.mean(axis=0)\n",
        "edges = np.arange(mean_times.min(), mean_times.max()+2, 3)\n",
        "bin_idx    = np.digitize(mean_times, edges) - 1\n",
        "n_bins     = len(edges)-1\n",
        "\n",
        "# Aggregate SHAP per bin (sum across frames)\n",
        "shap_bins = np.zeros((len(X_full), n_bins))\n",
        "for b in range(n_bins):\n",
        "    mask = (bin_idx == b)\n",
        "    if mask.any():\n",
        "        shap_bins[:, b] = shap_curves[:, mask].sum(axis=1)\n",
        "\n",
        "# Xompute “binned feature values” for coloring\n",
        "feat_bins = np.zeros((len(X_full), n_bins))\n",
        "for b in range(n_bins):\n",
        "    mask = (bin_idx == b)\n",
        "    if mask.any():\n",
        "        feat_bins[:, b] = datapoint_data[:, mask].mean(axis=1)\n",
        "\n",
        "# Stack together and build feature names\n",
        "X_agg_shap = np.hstack([shap_bins, shap_rf])\n",
        "X_agg_feat = np.hstack([feat_bins, feature_data])\n",
        "\n",
        "bin_labels = [f\"{int(edges[i])}–{int(edges[i+1])}min\"\n",
        "              for i in range(n_bins)]\n",
        "rf_labels  = rf_feats\n",
        "all_labels = bin_labels + rf_labels\n"
      ],
      "metadata": {
        "id": "kgKJZs0MTpBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bar plot**"
      ],
      "metadata": {
        "id": "maWWY8DW1wXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(\n",
        "    X_agg_shap,\n",
        "    features       = X_agg_feat,\n",
        "    feature_names  = all_labels,\n",
        "    plot_type      = \"bar\",\n",
        "    max_display    = 15\n",
        ")\n",
        "\n",
        "\n",
        "mean_rf  = np.mean(rf_model.predict_proba(feature_data)[:,1])\n",
        "mean_ens = np.mean(ens_model.predict_proba(X_full[bg_idx])[:,1])\n",
        "print(\"RF baseline:\", mean_rf, \"Ensemble baseline:\", mean_ens)"
      ],
      "metadata": {
        "id": "r6A13PDkwHCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Boxplot, donut chart and histogram**"
      ],
      "metadata": {
        "id": "IKTp5CmeybEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "raw_mass = np.sum(np.abs(shap_bins), axis=1)\n",
        "rf_mass  = np.sum(np.abs(shap_rf), axis=1)\n",
        "frac_curve = raw_mass / (raw_mass + rf_mass)\n",
        "\n",
        "# —— Boxplot —— #\n",
        "plt.figure()\n",
        "plt.boxplot(frac_curve, vert=False)\n",
        "plt.xlabel(\"Fraction of |SHAP| from raw-curve leg\")\n",
        "plt.title(\"Per-patient distribution of curve-leg contribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# —— Donut chart —— #\n",
        "total_raw = raw_mass.sum()\n",
        "total_rf  = rf_mass.sum()\n",
        "sizes     = [total_raw, total_rf]\n",
        "labels    = [\"Datapoint leg\", \"Feature leg\"]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.pie(sizes,\n",
        "       labels=labels,\n",
        "       autopct=\"%1.1f%%\",\n",
        "       startangle=90,\n",
        "       wedgeprops={\"width\": 0.3})\n",
        "ax.set_aspect(\"equal\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Histogram of how often the bins matter:\n",
        "plt.figure()\n",
        "plt.hist(frac_curve, bins=20)\n",
        "plt.xlabel(\"Fraction of total |SHAP| coming from time-bins\")\n",
        "plt.ylabel(\"Number of patients\")\n",
        "plt.title(\"How often the ensemble leans on its raw-curve leg\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "total_bins = raw_mass.sum()\n",
        "total_rf   = rf_mass.sum()\n",
        "print(\"Overall raw‐curve vs RF contribution:\",\n",
        "      total_bins/(total_bins+total_rf),\n",
        "      total_rf/(total_bins+total_rf))\n",
        "\n"
      ],
      "metadata": {
        "id": "C20RubQOfHc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sizes  = [100 - 57.5, 57.5]            # [42.5, 57.5]\n",
        "labels = [\"Datapoint leg\", \"Feature leg\"]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.pie(\n",
        "    sizes,\n",
        "    labels=labels,\n",
        "    autopct=\"%1.1f%%\",               # will show exactly 42.5% and 57.5%\n",
        "    startangle=90,\n",
        "    wedgeprops={\"width\": 0.3}\n",
        ")\n",
        "ax.set_aspect(\"equal\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RDWdPCazezfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dot plot**"
      ],
      "metadata": {
        "id": "udsrg02I100e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(\n",
        "    X_agg_shap,\n",
        "    features       = X_agg_feat,\n",
        "    feature_names  = all_labels,\n",
        "    plot_type      = \"dot\",\n",
        "    max_display    = 15\n",
        ")"
      ],
      "metadata": {
        "id": "rFnE_NCw148o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}